{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"GRASP: GRAph-oriented Synthetic data generation Pipeline <p>Framework to easily generate complex synthetic data pipelines by visualizing and configuring the pipeline as a computational graph. langgraph is used as the underlying graph configuration/execution library. Refer to langraph examples to get a sense of the different kinds of computational graph which can be configured."},{"location":"#introduction","title":"Introduction","text":"<p>GraSP Framework is created to generate synthetic data. As it is a complex process to define the flow, this design simplifies the synthetic data generation process. GraSP platform will support the following: - Defining the seed data configuration - Define a task, which involves graph node configuration, flow between nodes and conditions between the node - Define the output location to dump the generated data</p> <p>Seed data can be pulled from either Huggingface or file system. Once the seed data is loaded, GraSP platform allows datagen users to write any data processing using the data transformation module. When the data is ready, users can define the data flow with various types of nodes. A node can also be a subgraph defined in another yaml file.</p> <p>Each node can be defined with preprocessing, post processing, and LLM prompt with model parameters. Prompts can use seed data as python template keys. Edges define the flow between nodes, which can be conditional or non-conditional, with support for parallel and one-to-many flows.</p> <p>At the end, generated data is collected in the graph state for a specific record, processed further to generate the final dictionary to be written to the configured data sink.</p> <p></p>"},{"location":"#components","title":"Components","text":"<p>GraSP supports extendability and ease of implementation\u2014most tasks are defined as graph configuration YAML files. Each task consists of two major components: a graph configuration and Python code to define conditions and processors.</p> <p>A node is defined by the node module, supporting types like LLM call, multiple LLM call, lambda node, and sampler node. LLM-based nodes require a model configured in <code>models.yaml</code> and runtime parameters. Sampler nodes pick random samples from static YAML lists. For custom node types, you can implement new nodes in the platform.</p> <p>You can also define connections (edges) between nodes, which control conditional or parallel data flow.</p> <p>As of now, LLM inference is supported for TGI, vLLM, Azure, Azure OpenAI, Ollama and Triton compatible servers. Model deployment is external and configured in <code>models.yaml</code>.</p> <p></p>"},{"location":"#documentation","title":"Documentation","text":"<p>For a complete reference of all YAML configuration options, node/edge types, data sources and sinks, output mapping, schema validation, and advanced features:</p> <p>\ud83d\udc49 Documentation</p> <p>\ud83d\udc49 Getting Started</p> <p>\ud83d\udc49 Concepts</p> <p>\ud83d\udc49 GraSP Library</p>"},{"location":"#license","title":"License","text":"<p>[Your license here]</p> <p>Questions? Open an issue or discussion! Contributions are welcome.</p>"},{"location":"grasp_library/","title":"GraSP Library Documentation","text":""},{"location":"grasp_library/#overview","title":"Overview","text":"<p>The GraSP Library provides a high-level Python interface for building and executing complex data processing workflows with LLMs, agents, and advanced orchestration features. This library enables both programmatic workflow creation and existing YAML task execution with override capabilities.</p>"},{"location":"grasp_library/#key-features","title":"Key Features","text":"<ul> <li>Workflow Builder - Programmatic workflow construction with method chaining</li> <li>Multiple Node Types - LLM, Multi-LLM, Agent, Lambda, Weighted Sampler, Subgraph nodes</li> <li>Configuration Overrides - Dynamic runtime modifications with dot notation</li> <li>Data Processing - Multi-format support (JSON, JSONL, CSV, HuggingFace datasets)</li> <li>Advanced Features - Quality tagging, OASST mapping, resumable execution</li> <li>Callable Support - Pass Python functions, classes, and methods directly</li> </ul>"},{"location":"grasp_library/#installation","title":"Installation","text":"<pre><code>chmod +x setup_grasp_lib.sh\n./setup_grasp_lib.sh\n</code></pre>"},{"location":"grasp_library/#quick-start","title":"Quick Start","text":""},{"location":"grasp_library/#basic-workflow","title":"Basic Workflow","text":"<pre><code>import grasp\n\n# Simple text processing workflow\nworkflow = grasp.Workflow() \\\n    .source(\"data.json\") \\\n    .llm(\"gpt-4o\", \"Rewrite this text: {text}\") \\\n    .sink(\"output.json\") \\\n    .run()\n</code></pre>"},{"location":"grasp_library/#override-existing-tasks","title":"Override Existing Tasks","text":"<pre><code># Execute existing YAML task with runtime modifications\nworkflow = grasp.Workflow(\"my_existing_task\") \\\n    .override_prompt(\"generate_answer\", \"user\", \"Solve: {question}\", index=1) \\\n    .override_model(\"analyzer\", \"gpt-4o\", temperature=0.8) \\\n    .run(num_records=100)\n</code></pre>"},{"location":"grasp_library/#api-reference","title":"API Reference","text":""},{"location":"grasp_library/#workflow-class","title":"Workflow Class","text":"<p>The main entry point for creating and executing workflows.</p>"},{"location":"grasp_library/#constructor","title":"Constructor","text":"<pre><code>grasp.Workflow(name: Optional[str] = None)\n</code></pre> <p>Parameters: - <code>name</code> (str, optional): Workflow name. Auto-generated if not provided.</p>"},{"location":"grasp_library/#core-methods","title":"Core Methods","text":"Method Description Returns <code>source(source)</code> Set data source <code>Workflow</code> <code>sink(sink)</code> Set data sink <code>Workflow</code> <code>llm(model, prompt, **kwargs)</code> Add LLM node <code>Workflow</code> <code>agent(model, tools, prompt, **kwargs)</code> Add agent node <code>Workflow</code> <code>lambda_func(func, output, **kwargs)</code> Add lambda node <code>Workflow</code> <code>run(**kwargs)</code> Execute workflow <code>Any</code>"},{"location":"grasp_library/#node-types","title":"Node Types","text":""},{"location":"grasp_library/#llm-nodes","title":"LLM Nodes","text":"<p>Process text using Large Language Models.</p>"},{"location":"grasp_library/#basic-usage","title":"Basic Usage","text":"<pre><code># Simple LLM node\nworkflow.llm(\"gpt-4o\", \"Summarize: {text}\")\n</code></pre>"},{"location":"grasp_library/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>workflow.llm(\n    model={\n        \"name\": \"gpt-4o\", \n        \"parameters\": {\"temperature\": 0.7, \"max_tokens\": 2000}\n    },\n    prompt=[\n        {\"system\": \"You are an expert analyst\"},\n        {\"user\": \"Analyze: {data}\"}\n    ],\n    pre_process=preprocess_function,\n    post_process=postprocess_function\n)\n</code></pre>"},{"location":"grasp_library/#multi-llm-nodes","title":"Multi-LLM Nodes","text":"<p>Compare responses from multiple models simultaneously.</p> <pre><code>workflow.multi_llm(\n    models={\n        \"gpt4\": \"gpt-4o\",\n        \"claude\": \"claude-3-sonnet\",\n        \"local\": {\"name\": \"llama-3-70b\", \"endpoint\": \"localhost:8000\"}\n    },\n    prompt=\"Generate a story about: {topic}\"\n)\n</code></pre>"},{"location":"grasp_library/#agent-nodes","title":"Agent Nodes","text":"<p>LLM agents with tool access for autonomous task execution.</p> <pre><code>def calculator(expression: str) -&gt; str:\n    \"\"\"Evaluate mathematical expressions\"\"\"\n    return str(eval(expression))\n\ndef web_search(query: str) -&gt; dict:\n    \"\"\"Search the web for information\"\"\"\n    return {\"results\": f\"Search results for: {query}\"}\n\n# Agent with callable tools\nworkflow.agent(\n    model=\"gpt-4o\",\n    tools=[calculator, web_search],\n    prompt=\"Help solve: {problem}\",\n    chat_history=True\n)\n</code></pre>"},{"location":"grasp_library/#lambda-nodes","title":"Lambda Nodes","text":"<p>Custom processing functions and classes.</p>"},{"location":"grasp_library/#using-classes","title":"Using Classes","text":"<pre><code>from grasp.core.graph.functions.lambda_function import LambdaFunction\n\nclass TextProcessor(LambdaFunction):\n    @staticmethod\n    def apply(lambda_node_dict: dict, state: GraspState):\n        text = state.get(\"text\", \"\").strip().lower()\n        return {\n            **state, \n            \"processed_text\": text, \n            \"word_count\": len(text.split())\n        }\n\nworkflow.lambda_func(TextProcessor, output=\"processed_data\")\n</code></pre>"},{"location":"grasp_library/#using-functions","title":"Using Functions","text":"<pre><code>def extract_keywords(data):\n    \"\"\"Extract keywords from text\"\"\"\n    words = data[\"text\"].split()\n    return {\"keywords\": [w for w in words if len(w) &gt; 5]}\n\nworkflow.lambda_func(extract_keywords, output=\"keywords\")\n</code></pre>"},{"location":"grasp_library/#using-callable-objects","title":"Using Callable Objects","text":"<pre><code>class DataValidator:\n    def __init__(self, config):\n        self.config = config\n\n    def __call__(self, data):\n        score = len(data[\"text\"]) / 100\n        return {\n            \"quality_score\": min(1.0, score), \n            \"is_valid\": score &gt; self.config[\"threshold\"]\n        }\n\nvalidator = DataValidator({\"threshold\": 0.5})\nworkflow.lambda_func(validator, output=\"validation\")\n</code></pre>"},{"location":"grasp_library/#configuration-system","title":"Configuration System","text":""},{"location":"grasp_library/#universal-override-method","title":"Universal Override Method","text":"<p>Modify any configuration parameter using dot notation paths.</p> <pre><code>workflow = grasp.Workflow(\"existing_task\") \\\n    .override(\"graph_config.nodes.llm_1.model.parameters.temperature\", 0.9) \\\n    .override(\"graph_config.nodes.llm_1.prompt.0.system\", \"New instructions\") \\\n    .override(\"data_config.source.repo_id\", \"new/dataset\") \\\n    .run()\n</code></pre>"},{"location":"grasp_library/#helper-methods","title":"Helper Methods","text":""},{"location":"grasp_library/#model-overrides","title":"Model Overrides","text":"<pre><code>workflow.override_model(\n    node_name=\"analyzer\", \n    model_name=\"gpt-4o\", \n    temperature=0.8, \n    max_tokens=1500\n)\n</code></pre>"},{"location":"grasp_library/#prompt-overrides","title":"Prompt Overrides","text":"<pre><code>workflow.override_prompt(\n    node_name=\"generator\", \n    role=\"system\", \n    content=\"You are a helpful assistant\", \n    index=0\n)\n</code></pre>"},{"location":"grasp_library/#example","title":"Example","text":"<pre><code># Override glaive_code_assistant task configuration\nworkflow = grasp.Workflow(\"examples/glaive_code_assistant\") \\\n    .override_prompt(\n        \"generate_answer\", \n        \"user\", \n        \"Solve step by step: {question}\", \n        index=1\n    ) \\\n    .override_prompt(\n        \"critique_answer\", \n        \"system\", \n        \"Be thorough in your code review\", \n        index=0\n    ) \\\n    .override_model(\"generate_answer\", \"gpt-4o\", temperature=0.2) \\\n    .run(num_records=50)\n</code></pre>"},{"location":"grasp_library/#graph-builder","title":"Graph Builder","text":"<p>For complex workflows requiring explicit control flow.</p>"},{"location":"grasp_library/#basic-construction","title":"Basic Construction","text":"<pre><code># Create graph with callable conditions\ndef quality_gate(state):\n    \"\"\"Route based on quality score\"\"\"\n    return \"approved\" if state.get(\"quality_score\", 0) &gt; 0.8 else \"needs_review\"\n\ngraph = grasp.create_graph(\"advanced_workflow\")\n\n# Add and configure nodes\nanalyzer = graph.add_llm_node(\"analyzer\", \"gpt-4o\") \\\n    .system_prompt(\"Analyze the following content\") \\\n    .user_prompt(\"Content: {text}\") \\\n    .temperature(0.7)\n\nreviewer = graph.add_agent_node(\"reviewer\", \"gpt-4o\") \\\n    .tools([fact_checker, web_search]) \\\n    .system_prompt(\"Review and verify the analysis\")\n\n# Define control flow\ngraph.sequence(\"analyzer\", \"reviewer\") \\\n    .add_conditional_edge(\n        \"reviewer\",\n        condition=quality_gate,  # Callable condition\n        path_map={\n            \"approved\": \"END\", \n            \"needs_review\": \"analyzer\"\n        }\n    )\n\n# Execute\nresult = graph.set_source(\"documents.json\") \\\n    .set_sink(\"analyzed_docs.json\") \\\n    .enable_quality_tagging() \\\n    .run(num_records=100)\n</code></pre>"},{"location":"grasp_library/#processor-classes","title":"Processor Classes","text":""},{"location":"grasp_library/#pre-processors","title":"Pre-Processors","text":"<p>Modify state before node execution.</p> <pre><code>from grasp.core.graph.functions.node_processor import NodePreProcessor\n\nclass InputValidator(NodePreProcessor):\n    def apply(self, state: GraspState) -&gt; GraspState:\n        # Validate required fields\n        if \"text\" not in state or not state[\"text\"]:\n            state[\"text\"] = \"[MISSING_TEXT]\"\n\n        # Add metadata\n        state[\"validated\"] = True\n        state[\"timestamp\"] = datetime.now().isoformat()\n\n        return state\n\n# Use in workflow\nworkflow.llm(\n    \"gpt-4o\", \n    \"Process: {text}\", \n    pre_process=InputValidator\n)\n</code></pre>"},{"location":"grasp_library/#post-processors","title":"Post-Processors","text":"<p>Process responses after node execution.</p> <pre><code>from grasp.core.graph.functions.node_processor import NodePostProcessor\n\nclass ResponseFormatter(NodePostProcessor):\n    def apply(self, response: GraspMessage) -&gt; GraspState:\n        content = response.message.content\n\n        return {\n            \"formatted_response\": content.strip(),\n            \"response_length\": len(content),\n            \"has_content\": len(content.strip()) &gt; 0,\n            \"word_count\": len(content.split())\n        }\n\n# Use in workflow\nworkflow.llm(\n    \"gpt-4o\", \n    \"Analyze: {text}\", \n    post_process=ResponseFormatter\n)\n</code></pre>"},{"location":"grasp_library/#post-processors-with-state","title":"Post-Processors with State","text":"<p>Access both response and original state.</p> <pre><code>from grasp.core.graph.functions.node_processor import NodePostProcessorWithState\n\nclass QualityAnalyzer(NodePostProcessorWithState):\n    def apply(self, response: GraspMessage, state: GraspState) -&gt; GraspState:\n        content = response.message.content\n        original_text = state.get(\"text\", \"\")\n\n        # Calculate quality metrics\n        quality_score = self._calculate_quality(original_text, content)\n\n        return {\n            **state,  # Preserve original state\n            \"processed_response\": content,\n            \"quality_score\": quality_score,\n            \"is_high_quality\": quality_score &gt; 0.7\n        }\n\n    def _calculate_quality(self, original, response):\n        # Your quality calculation logic\n        return 0.85\n</code></pre>"},{"location":"grasp_library/#data-sources-and-sinks","title":"Data Sources and Sinks","text":""},{"location":"grasp_library/#source-options","title":"Source Options","text":"Type Example Description File <code>workflow.source(\"data.json\")</code> Local files (JSON, JSONL, CSV) Memory <code>workflow.source([{\"text\": \"sample\"}])</code> In-memory data HuggingFace <code>workflow.source({\"type\": \"hf\", \"repo_id\": \"dataset/name\"})</code> HF datasets"},{"location":"grasp_library/#advanced-data-sources","title":"Advanced Data Sources","text":"<pre><code>from grasp import DataSource, DataSink\n\n# Memory data source\ngraph.set_source(DataSource.memory([\n    {\"id\": 1, \"text\": \"Sample text\"},\n    {\"id\": 2, \"text\": \"Another sample\"}\n]))\n\n# File data sink\ngraph.set_sink(DataSink.disk(\"output/results.jsonl\"))\n</code></pre>"},{"location":"grasp_library/#advanced-features","title":"Advanced Features","text":""},{"location":"grasp_library/#quality-control","title":"Quality Control","text":"<pre><code>workflow.quality_tagging(\n    enabled=True,\n    config={\n        \"metrics\": [\"coherence\", \"relevance\", \"factuality\"],\n        \"threshold\": 0.8,\n        \"judge_model\": \"gpt-4o\"\n    }\n)\n</code></pre>"},{"location":"grasp_library/#oasst-conversation-mapping","title":"OASST Conversation Mapping","text":"<pre><code>workflow.oasst_mapping(\n    enabled=True,\n    config={\n        \"required\": \"yes\",\n        \"format\": \"conversation\"\n    }\n)\n</code></pre>"},{"location":"grasp_library/#resumable-execution","title":"Resumable Execution","text":"<pre><code>workflow.resumable(True).run(\n    num_records=10000,\n    batch_size=50,\n    checkpoint_interval=1000,\n    resume=True  # Resume from last checkpoint\n)\n</code></pre>"},{"location":"grasp_library/#examples","title":"Examples","text":""},{"location":"grasp_library/#content-analysis-pipeline","title":"Content Analysis Pipeline","text":"<pre><code>def sentiment_analyzer(data):\n    \"\"\"Analyze text sentiment\"\"\"\n    text = data.get(\"text\", \"\")\n    # Your sentiment analysis implementation\n    return {\"sentiment\": \"positive\", \"confidence\": 0.85}\n\ndef topic_extractor(data):\n    \"\"\"Extract topics from text\"\"\"  \n    # Your topic extraction implementation\n    return {\"topics\": [\"technology\", \"AI\"], \"primary_topic\": \"technology\"}\n\n# Comprehensive analysis workflow\nworkflow = grasp.Workflow(\"content_analysis\") \\\n    .source(\"articles.json\") \\\n    .lambda_func(sentiment_analyzer, output=\"sentiment_data\") \\\n    .lambda_func(topic_extractor, output=\"topic_data\") \\\n    .llm(\"gpt-4o\", [\n        {\"system\": \"Create a comprehensive summary report\"},\n        {\"user\": \"Article: {text}\\nSentiment: {sentiment}\\nTopics: {topics}\"}\n    ]) \\\n    .quality_tagging(enabled=True) \\\n    .sink(\"analysis_results.json\") \\\n    .run()\n</code></pre>"},{"location":"grasp_library/#research-assistant","title":"Research Assistant","text":"<pre><code>def search_papers(query: str) -&gt; list:\n    \"\"\"Search academic papers\"\"\"\n    return [{\"title\": f\"Paper on {query}\", \"abstract\": \"Research findings...\"}]\n\ndef fact_check(claim: str) -&gt; dict:\n    \"\"\"Verify factual claims\"\"\"\n    return {\"claim\": claim, \"verified\": True, \"confidence\": 0.9}\n\n# Multi-model research workflow\nworkflow = grasp.Workflow(\"research_assistant\") \\\n    .source(\"research_questions.json\") \\\n    .agent(\n        model=\"gpt-4o\",\n        tools=[search_papers, fact_check],\n        prompt=\"Research thoroughly: {question}\",\n        chat_history=True\n    ) \\\n    .multi_llm(\n        models={\n            \"summarizer\": \"gpt-4o\", \n            \"reviewer\": \"claude-3-sonnet\"\n        },\n        prompt=\"Synthesize research findings: {messages}\"\n    ) \\\n    .sink(\"research_reports.json\") \\\n    .run()\n</code></pre>"},{"location":"grasp_library/#api-reference_1","title":"API Reference","text":""},{"location":"grasp_library/#return-types","title":"Return Types","text":"Method Return Type Description <code>workflow.run()</code> <code>List[Dict]</code> or <code>Any</code> Processed results <code>graph.build()</code> <code>ExecutableGraph</code> Built graph object <code>graph.run()</code> <code>List[Dict]</code> or <code>Any</code> Execution results"},{"location":"installation/","title":"Installation","text":"<ol> <li>Create an environment with Python <code>3.11</code> or higher, using <code>pyenv</code> or <code>conda</code>.<ul> <li>For <code>pyenv</code>, use the following commands:     <pre><code>brew install pyenv\npyenv install 3.11\npyenv local 3.11\n</code></pre></li> <li>To set up your PATH automatically every time you open a shell session, add this to your .zshrc file:     <pre><code>eval \"$(pyenv init -)\"\n</code></pre></li> </ul> </li> <li>Clone the repository.</li> </ol> <pre><code>  git clone git@github.com:ServiceNow/grasp.git\n</code></pre> <p>[!IMPORTANT] If you have already cloned GraSP locally, follow the steps below to update your remote to point to the correct URL:  <pre><code>git remote set-url origin git@github.com:ServiceNow/grasp.git\ngit fetch --prune\n</code></pre></p> <ol> <li>Install poetry using the official guidelines.</li> <li>Run <code>poetry install</code>.</li> <li>(Optional) In your IDE, set your python interpreter to the poetry virtual environment.    Run <code>poetry run which python</code> to see the python interpreter path that you should add in your IDE.</li> </ol>"},{"location":"concepts/data_handler/","title":"Data Handlers","text":""},{"location":"concepts/data_handler/#components","title":"Components","text":""},{"location":"concepts/data_handler/#base-interface-datahandler","title":"Base Interface (<code>DataHandler</code>)","text":"<p>The abstract base class that defines the core interface for all data handlers:</p> <ul> <li><code>read()</code>: Read data from a source</li> <li><code>write()</code>: Write data to a destination</li> <li><code>get_files()</code>: List available files in the data source</li> </ul>"},{"location":"concepts/data_handler/#huggingface-handler","title":"HuggingFace Handler","text":"<p>Specializes in interacting with HuggingFace datasets, supporting:</p> <ul> <li>Reading from public/private datasets</li> <li>Streaming large datasets</li> <li>Sharded dataset handling</li> <li>Dataset card (README) management</li> <li>Multiple data splits</li> </ul>"},{"location":"concepts/data_handler/#file-system-handler","title":"File System Handler","text":"<p>Manages local file operations with support for:</p> <ul> <li>JSON, JSONL (JSON Lines), Parquet files</li> <li>Special data type handling (datetime, numpy arrays)</li> </ul>"},{"location":"concepts/data_handler/#configuration","title":"Configuration","text":"<p>The YAML data configuration consists of the following: - <code>data_config</code>: Defines data source and sink configurations with <code>source</code> and <code>sink</code> keys.  - The <code>source</code> key specifies the input data config, and the <code>sink</code> key specifies the output data config.</p>"},{"location":"concepts/data_handler/#basic-structure","title":"Basic Structure","text":"<pre><code>data_config:\n  source:\n    type: \"hf\"  # or \"disk\" for local filesystem\n    # source-specific configurations\n  sink:\n    type: \"hf\"  # or \"disk\" for local filesystem\n    # sink-specific configurations\n</code></pre>"},{"location":"concepts/data_handler/#use-cases","title":"Use Cases","text":""},{"location":"concepts/data_handler/#1-reading-from-huggingface-public-datasets","title":"1. Reading from HuggingFace Public Datasets","text":"<pre><code>data_config:\n  source:\n    type: \"hf\"\n    repo_id: \"google-research-datasets/mbpp\"\n    config_name: \"sanitized\"\n    split: [\"train\", \"validation\", \"prompt\"]\n</code></pre>"},{"location":"concepts/data_handler/#source-configuration","title":"Source Configuration","text":"<p>Configure your data source using <code>DataSourceConfig</code>:</p> <pre><code>from grasp.core.dataset.dataset_config import DataSourceConfig\n\n# For HuggingFace datasets\nhf_config = DataSourceConfig(\n    repo_id=\"datasets/dataset-name\",\n    config_name=\"default\",\n    split=\"train\",\n    token=\"your_hf_token\",  # Optional for private datasets\n    streaming=True,  # Optional for large datasets\n    shard=None  # Optional for sharded datasets\n)\n\n# For local files\nfile_config = DataSourceConfig(\n    file_path=\"/path/to/data.json\",\n    encoding=\"utf-8\",  # Optional, defaults to utf-8\n)\n</code></pre>"},{"location":"concepts/data_handler/#output-configuration","title":"Output Configuration","text":"<p>Configure your output destination using <code>OutputConfig</code>:</p> <pre><code>from grasp.core.dataset.dataset_config import OutputConfig\n\n# For HuggingFace datasets\nhf_output = OutputConfig(\n    repo_id=\"your-username/your-dataset\",\n    config_name=\"default\",\n    split=\"train\",\n    token=\"your_hf_token\",\n    private=True  # Optional, defaults to False\n)\n\n# For local files\nfile_output = OutputConfig(\n    encoding=\"utf-8\"  # Optional, defaults to utf-8\n)\n</code></pre>"},{"location":"concepts/data_handler/#usage-examples","title":"Usage Examples","text":""},{"location":"concepts/data_handler/#working-with-huggingface-datasets","title":"Working with HuggingFace Datasets","text":"<ol> <li>Reading from a public dataset:</li> </ol> <p>YAML: <pre><code>data_config:\n  source:\n    type: \"hf\"\n    repo_id: \"google-research-datasets/mbpp\"\n    config_name: \"sanitized\"\n    split: [\"train\", \"validation\", \"prompt\"]\n</code></pre></p> <p>Python:</p> <pre><code>from grasp.core.dataset.huggingface_handler import HuggingFaceHandler\nfrom grasp.core.dataset.dataset_config import DataSourceConfig\n\n# Configure source\nconfig = DataSourceConfig(\n    repo_id=\"databricks/databricks-dolly-15k\",\n    config_name=\"default\",\n    split=\"train\"\n)\n\n# Initialize handler\nhandler = HuggingFaceHandler(source_config=config)\n\n# Read data\ndata = handler.read()\n</code></pre> <ol> <li>Writing to your private dataset:</li> </ol> <p>YAML:</p> <pre><code>data_config:\n  sink:\n    type: \"hf\"\n    repo_id: \"your-username/your-dataset\"\n    config_name: \"custom_config\"\n    split: \"train\"\n    push_to_hub: true\n    private: true\n</code></pre> <p>Python:</p> <pre><code># Configure output\n\noutput_config = OutputConfig(\n    repo_id=\"your-username/your-dataset\",\n    config_name=\"default\",\n    split=\"train\",\n    token=\"your_hf_token\",\n    private=True\n)\n\nhandler = HuggingFaceHandler(output_config=output_config)\nhandler.write(data) \n</code></pre> <ol> <li>Working with sharded datasets:</li> </ol> <p>YAML:</p> <pre><code>data_config:\n  source:\n    type: \"hf\"\n    repo_id: \"large-dataset\"\n    shard:\n      regex: \"-.*\\\\.parquet$\"\n      index: [0, 1, 2]  # Only process first 3 shards\n</code></pre> <p>Python: </p> <pre><code>config = DataSourceConfig(\n    repo_id=\"large-dataset\",\n    shard={\"regex\": \"-.*\\\\.parquet$\", \"index\": [0, 1, 2]}  # Only process first 3 shards\n)\n\nhandler = HuggingFaceHandler(source_config=config)\nshard_files = handler.get_files()\n\nfor shard_path in shard_files:\n    shard_data = handler.read(path=shard_path)\n    # Process shard data\n</code></pre>"},{"location":"concepts/data_handler/#field-transformations","title":"Field Transformations","text":"<p>YAML: </p> <pre><code>data_config:\n  source:\n    type: \"hf\"\n    repo_id: \"dataset/name\"\n    transformations:\n      - transform: grasp.processors.data_transform.RenameFieldsTransform\n        params:\n          mapping:\n            old_field: new_field\n          overwrite: false\n</code></pre> <p>Python: </p> <pre><code>from grasp.processors.data_transform import RenameFieldsTransform\n\nconfig = DataSourceConfig(\n    repo_id=\"dataset/name\",\n    transformations=[\n        {\n            \"transform\": RenameFieldsTransform,\n            \"params\": {\n                \"mapping\": {\"old_field\": \"new_field\"},\n                \"overwrite\": False\n            }\n        }\n    ]\n)\n\nhandler = HuggingFaceHandler(source_config=config)\ndata = handler.read()\n</code></pre>"},{"location":"concepts/data_handler/#working-with-local-files","title":"Working with Local Files","text":"<ol> <li>Reading from JSON/Parquet/JSONL files:</li> </ol> <p>YAML:</p> <pre><code>data_config:\n  source:\n    type: \"disk\"\n    file_path: \"data/input.json\"   # also supports Parquet, JSONL\n</code></pre> <p>Python: </p> <pre><code>from data_handlers import FileHandler\n\nconfig = DataSourceConfig(file_path=\"/data/input.parquet\")\nhandler = FileHandler(source_config=config)\ndata = handler.read()\n</code></pre> <ol> <li>Writing to JSONL with custom encoding:</li> </ol> <p>YAML:</p> <pre><code>data_config:\n  sink:\n    type: \"disk\"\n    file_path: \"data/output.jsonl\"\n    encoding: \"utf-16\"\n</code></pre> <p>Python: </p> <pre><code>output_config = OutputConfig(encoding=\"utf-16\")\nhandler = FileHandler(output_config=output_config)\nhandler.write(data, path=\"/data/output.jsonl\")\n</code></pre>"},{"location":"concepts/data_mapper/","title":"Data Mapper","text":""},{"location":"concepts/data_mapper/#introduction","title":"Introduction","text":"<p>Data Mapper enables users to transform data from an output record generator into SFT or DPO format, based on configuration preferences.</p>"},{"location":"concepts/data_mapper/#key-features","title":"Key Features","text":"<ol> <li> <p>YAML-Based Transformation Control    Users can select the transformation format (SFT/DPO) by modifying the <code>required</code> and <code>type</code> keys under the <code>oasst_mapper</code> section in the configuration file.</p> </li> <li> <p>Automated Output Validation    Once a transformation type is chosen, the output is automatically schema-validated\u2014no manual validation required.</p> </li> </ol>"},{"location":"concepts/data_mapper/#default-behavior","title":"Default Behavior","text":"<p>By default, the Data Mapper is disabled. To enable it, add the following runtime argument:</p> <pre><code>--oasst True\n</code></pre> <p>The default configuration is specified in the <code>configuration.yaml</code> file:</p> <pre><code>oasst_mapper:\n  required: \"yes\"\n  type: \"sft\"\n  intermediate_writing: \"no\"\n</code></pre> <ul> <li><code>required</code>: Indicates whether transformation is mandatory (<code>\"yes\"</code>).</li> <li><code>type</code>: Defines the transformation format (<code>\"sft\"</code> or <code>\"dpo\"</code>).</li> <li><code>intermediate_writing</code>: Enables saving intermediate files for debugging (<code>\"yes\"</code> or <code>\"no\"</code>).</li> </ul>"},{"location":"concepts/data_mapper/#usage-illustration","title":"Usage Illustration","text":"<p>Given an input record like the following:</p> <pre><code>{\n  \"id\": 602,\n  \"conversation\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"How would you draft a Python function that identifies the initial recurring character in a specified string?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"def first_repeated_char(s):\\n    seen = set()\\n    for char in s:\\n        if char in seen:\\n            return char\\n        seen.add(char)\\n    return None\"\n    }\n  ],\n  \"taxonomy\": [{\"category\": \"Coding\", \"subcategory\": \"\"}],\n  \"annotation_type\": [\"mistral-large\"],\n  \"language\": \"en\",\n  \"tags\": [\"mbpp\", \"reannotate\", \"self-critique\"]\n}\n</code></pre>"},{"location":"concepts/data_mapper/#to-transform","title":"To Transform","text":"<p>You can run the Data Mapper in two modes:</p>"},{"location":"concepts/data_mapper/#1-default-mode","title":"1. Default Mode","text":"<p>Run with:</p> <pre><code>--oasst True\n</code></pre> <p>This uses the configuration specified in <code>configuration.yaml</code>:</p> <pre><code>oasst_mapper:\n  required: \"yes\"\n  type: \"sft\"\n  intermediate_writing: \"no\"\n</code></pre>"},{"location":"concepts/data_mapper/#2-overridden-mode","title":"2. Overridden Mode","text":"<p>To override the default behavior, modify the <code>output_config</code> section in your <code>graph_config.yaml</code> file like so:</p> <pre><code>output_config:\n  oasst_mapper:\n    required: \"yes\"\n    type: \"sft\"\n    intermediate_writing: \"yes\"\n</code></pre> <p>\ud83d\udca1 Set <code>intermediate_writing</code> to <code>\"yes\"</code> to store intermediate files before transformation (useful for debugging). If not provided or set to any other value, it defaults to <code>\"no\"</code>.</p>"},{"location":"concepts/data_mapper/#output-example","title":"Output Example","text":"<p>The transformed output will follow the selected format (e.g., SFT), like this:</p> <pre><code>[\n  {\n    \"conversation_id\": \"conv_8513aa73\",\n    \"message_id\": \"msg_1_59a96096\",\n    \"parent_id\": null,\n    \"root_message_id\": \"msg_1_59a96096\",\n    \"message_level\": 1,\n    \"role\": \"user\",\n    \"content\": \"...\",\n    ...\n  },\n  {\n    \"conversation_id\": \"conv_8513aa73\",\n    \"message_id\": \"msg_2_e8b221e8\",\n    \"parent_id\": \"msg_1_59a96096\",\n    \"root_message_id\": \"msg_1_59a96096\",\n    \"message_level\": 2,\n    \"role\": \"assistant\",\n    \"content\": \"...\",\n    ...\n  }\n]\n</code></pre>"},{"location":"concepts/data_mapper/#rules-for-using-data-mapping","title":"Rules for Using Data Mapping","text":"<ol> <li>To run in default mode, simply use the runtime flag <code>--oasst True</code>.</li> <li>If you want to override defaults, define <code>oasst_mapper</code> inside <code>output_config</code> in the config file.</li> <li>If <code>oasst_mapper</code> is defined, its keys must be:</li> <li><code>required</code>: must be <code>\"yes\"</code> or <code>\"no\"</code></li> <li><code>type</code>: must be <code>\"sft\"</code> or <code>\"dpo\"</code></li> <li><code>intermediate_writing</code>: optional, defaults to <code>\"no\"</code> if not <code>\"yes\"</code></li> <li>If keys are missing or values are invalid, exceptions will be raised.</li> </ol>"},{"location":"concepts/data_mapper/#additional-features","title":"Additional Features","text":"<p>Support for custom transformation types and custom validation schemas is planned in a future release.</p>"},{"location":"concepts/data_quality/","title":"Data Quality","text":""},{"location":"concepts/data_quality/#introduction","title":"Introduction","text":"<p>Data Quality enables users to add data quality metrics to the generated output records. This is particularly useful for evaluating the performance of models and ensuring that the generated data meets specific quality standards.</p>"},{"location":"concepts/data_quality/#key-features","title":"Key Features","text":"<ol> <li>YAML-Based Configuration    Users can specify the data quality metrics they want to include in the output records by modifying the <code>data_quality</code> section in the configuration file.</li> <li>Automatic injection of quality in OASST data     The quality metrics can be automatically injected directly into output records following the OASST format without any additional configuration.</li> </ol>"},{"location":"concepts/data_quality/#default-behavior","title":"Default Behavior","text":"<p>By default, the Data Quality feature is disabled. To enable it, add the following runtime argument:</p> <pre><code>--quality True\n````\n\nThe default configuration is specified in the `configuration.yaml` file:\n\n```yaml\n    data_quality:\n      tasks:\n          - name: \"conversation_pretokenization\"\n            params:\n              hf_chat_template_model_id: \"Qwen/Qwen2.5-32B-Instruct\"\n          - name: \"metadata_tagging\"\n          - name: \"llm_based_quality\"\n          - name: \"data_characteristics\"\n            params:\n              model: \"Qwen/Qwen2.5-32B-Instruct\"\n          - name: \"language_tagging\"\n          - name: \"lexical_diversity\"\n          - name: \"ppl_score\"\n            params:\n              model_config:\n                model_type: \"vllm\"  # or \"tgi\"\n                url: # model url\n                model_serving_name: \"qwen-32B\"\n</code></pre>"},{"location":"concepts/data_quality/#configuration-parameters","title":"Configuration Parameters","text":"<ul> <li><code>skip_failed_tasks</code>: When set to <code>true</code>, the processor will continue execution even if individual data quality tasks fail. When <code>false</code>, the processor will stop on the first task failure. Default is <code>true</code>.</li> <li><code>tasks</code>: A list of tasks to be performed for data quality assessment. Each task can have its own parameters.</li> </ul>"},{"location":"concepts/data_quality/#available-tasks","title":"Available Tasks","text":"<p>The following data quality tasks are available: 1. conversation_pretokenization: Performs pre-tokenization on conversation data. 2. metadata_tagging: Tags records with metadata. 3. llm_based_quality: Assesses quality using LLM-based metrics. 4. data_characteristics: Extracts characteristics from data like token counts and turn numbers. 5. language_tagging: Identifies and tags the language of the text. 6. lexical_diversity: Computes lexical diversity metrics (TTR - Type-Token Ratio). 7. ppl_score: Calculates perplexity scores for text quality assessment.</p> <pre><code>`Note`: You need to host the model first and share the URL in the parameters. The model should be hosted either in a vLLM or TGI server.\n</code></pre> <ol> <li> <p>reward_score: Calculates reward scores from the reward model. (Offline setup)</p> <p><code>Note</code>: This task requires running the codebase in an environment with a GPU, or using a reward model compatible with CPU execution.</p> </li> </ol>"},{"location":"concepts/data_quality/#usage-illustration","title":"Usage Illustration","text":"<p>You can run the Data Quality in two modes:</p>"},{"location":"concepts/data_quality/#1-default-mode","title":"1. Default Mode","text":"<p>Run with:</p> <pre><code>--quality True\n</code></pre> <p>This uses the configuration specified in <code>configuration.yaml</code></p>"},{"location":"concepts/data_quality/#2-overridden-mode","title":"2. Overridden Mode","text":"<p>To override the default behavior, modify the <code>output_config</code> section in your <code>graph_config.yaml</code> file like so:</p> <p><pre><code>output_config:\n    data_quality:\n        tasks:\n            - name: \"metadata_tagging\"\n            - name: \"llm_based_quality\"\n</code></pre> This will make the data quality to run only for the specified tasks.</p> <p>The input data can be in conversation or in OASST format. The Data Quality feature will automatically inject the specified quality metrics into the output records.</p>"},{"location":"concepts/data_quality/#output-example","title":"Output Example","text":"<p>The example below is for default mode. The overridden mode will have the same format but with different data quality metrics based on the configuration</p>"},{"location":"concepts/data_quality/#conversation-data","title":"Conversation data","text":"<pre><code>[\n  {\n        \"id\": \"3056af5d\",\n        \"conversation\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"I have a field that contains a string of numbers like \\\"2002 2005 2001 2006 2008 2344\\\". I know how to select a substring using the `SELECT substr(years,1,4)  FROM TABLE` query, but I'm struggling to check each substring individually. My goal is to find the row that contains the number closest to 0. Is there a way to achieve this using SQL?\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"To find the number closest to 0 in a string of space-separated numbers within a SQL query, you can use a combination of string functions and a common table expression (CTE) or a subquery to split the string and then find the minimum absolute value. Here's an example using a CTE in SQLite:\\n\\n```sql\\nWITH RECURSIVE SplitNumbers AS (\\n  SELECT\\n    CAST(SUBSTR(years, 1, INSTR(years || ' ', ' ') - 1) AS INTEGER) AS number,\\n    SUBSTR(years, INSTR(years || ' ', ' ') + 1) AS remaining\\n  FROM\\n    your_table\\n  UNION ALL\\n  SELECT\\n    CAST(SUBSTR(remaining, 1, INSTR(remaining || ' ', ' ') - 1) AS INTEGER),\\n    SUBSTR(remaining, INSTR(remaining || ' ', ' ') + 1)\\n  FROM\\n    SplitNumbers\\n  WHERE\\n    remaining != ''\\n)\\nSELECT\\n  number\\nFROM\\n  SplitNumbers\\nORDER BY\\n  ABS(number)\\nLIMIT 1;\\n```\\n\\nReplace `your_table` with the actual name of your table and `years` with the actual column name. This query will split the string into individual numbers, convert them to integers, and then find the number with the smallest absolute value.\"\n            }\n        ],\n        \"data_quality_metadata\": {\n            \"data_taxonomy\": {\n                \"category\": \"Code Writing\",\n                \"subcategory\": \"\",\n                \"instruction_tags\": [\n                    \"SQL Query Help\",\n                    \"Data Processing\",\n                    \"Problem Solving\"\n                ]\n            },\n            \"data_quality\": {\n                \"llm_based\": {\n                    \"question_quality_score\": 4,\n                    \"explanation_question_quality\": \"The prompt is well-articulated and clear, with a specific goal of finding the number closest to 0 in a string of numbers using SQL. It demonstrates a good understanding of SQL functions and expresses the user's struggle effectively. The grammar is correct, and the question is relevant to SQL operations, though it could benefit from more depth in terms of exploring potential solutions.\",\n                    \"response_quality\": {\n                        \"instruction_following\": 5,\n                        \"explanation_instruction_following\": \"The response follows the instruction to find the number closest to 0 from a string of numbers using SQL.\",\n                        \"accuracy\": 5,\n                        \"explanation_accuracy\": \"The response accurately provides a SQL query that splits the string into individual numbers and finds the number closest to 0.\",\n                        \"relevance\": 5,\n                        \"explanation_relevance\": \"The response is directly relevant to the query, focusing on solving the problem of finding the number closest to 0.\",\n                        \"clarity\": 5,\n                        \"explanation_clarity\": \"The response is clear and well-structured, with a step-by-step explanation of the SQL query.\",\n                        \"completeness\": 5,\n                        \"explanation_completeness\": \"The response is complete, providing a full SQL solution and instructions on how to adapt it to the user's table and column names.\",\n                        \"conciseness\": 5,\n                        \"explanation_conciseness\": \"The response is concise, providing all necessary information without unnecessary details.\",\n                        \"overall_score\": 5.0\n                    }\n                }\n            },\n            \"data_characteristics\": {\n                \"num_turns\": 2,\n                \"num_tokens\": 371,\n                \"num_input_tokens\": 92,\n                \"num_target_tokens\": 279\n            }\n        }\n    }\n]\n</code></pre>"},{"location":"concepts/data_quality/#oasst-data","title":"OASST data","text":"<pre><code>[\n  {\n    \"conversation_id\": \"3056af5d\",\n    \"message_id\": \"msg_1_59a96096\",\n    \"parent_id\": null,\n    \"root_message_id\": \"msg_1_59a96096\",\n    \"message_level\": 1,\n    \"role\": \"user\",\n    \"content\": \"...\",\n    ...\n  },\n  {\n    \"conversation_id\": \"conv_8513aa73\",\n    \"message_id\": \"msg_2_e8b221e8\",\n    \"parent_id\": \"msg_1_59a96096\",\n    \"root_message_id\": \"msg_1_59a96096\",\n    \"message_level\": 2,\n    \"role\": \"assistant\",\n    \"content\": \"...\",\n    \"quality\": {\n        \"quality_characteristics_LLM_based_question_quality_score\": 4,\n        \"quality_characteristics_LLM_based_explanation_question_quality\": \"The prompt is well-articulated and clear, with a specific goal of finding the number closest to 0 in a string of numbers using SQL. It demonstrates a good understanding of SQL functions and expresses the user's struggle effectively. The grammar is correct, and the question is relevant to SQL operations, though it could benefit from more depth in terms of exploring potential solutions.\",\n        \"quality_characteristics_LLM_based_response_quality_instruction_following\": 5,\n        \"quality_characteristics_LLM_based_response_quality_explanation_instruction_following\": \"The response follows the instructions by providing a SQL query to find the number closest to 0 from a string of numbers.\",\n        \"quality_characteristics_LLM_based_response_quality_accuracy\": 5,\n        \"quality_characteristics_LLM_based_response_quality_explanation_accuracy\": \"The response accurately uses SQL functions and CTEs to split the string and find the minimum absolute value, which aligns with the query requirements.\",\n        \"quality_characteristics_LLM_based_response_quality_relevance\": 5,\n        \"quality_characteristics_LLM_based_response_quality_explanation_relevance\": \"The response is directly relevant to the query, focusing on the task of finding the number closest to 0 using SQL.\",\n        \"quality_characteristics_LLM_based_response_quality_clarity\": 5,\n        \"quality_characteristics_LLM_based_response_quality_explanation_clarity\": \"The response is clear and well-structured, with a step-by-step explanation of the SQL query process.\",\n        \"quality_characteristics_LLM_based_response_quality_completeness\": 5,\n        \"quality_characteristics_LLM_based_response_quality_explanation_completeness\": \"The response is complete, addressing all parts of the query and providing a full SQL solution.\",\n        \"quality_characteristics_LLM_based_response_quality_conciseness\": 5,\n        \"quality_characteristics_LLM_based_response_quality_explanation_conciseness\": \"The response is concise, providing necessary information without unnecessary verbosity.\",\n        \"quality_characteristics_LLM_based_response_quality_overall_score\": 5.0\n    },\n    \"data_characteristics\": {\n        \"num_turns\": 2,\n        \"num_tokens\": 381,\n        \"num_input_tokens\": 92,\n        \"num_target_tokens\": 289\n    },\n    ...\n  }\n]\n</code></pre>"},{"location":"concepts/data_quality/#rules-for-using-data-quality","title":"Rules for Using Data Quality","text":"<ol> <li>Default Behavior: The default behavior is to run the data quality tasks as specified in the <code>configuration.yaml</code> file.</li> <li>Overridden Mode: You can override the default behavior by modifying the <code>output_config</code> section in your <code>graph_config.yaml</code> file. This will allow you to specify which tasks to run and their parameters.</li> <li>Input Data Format: The input data can be in conversation or OASST format. The Data Quality feature will automatically inject the specified quality metrics into the output records.</li> <li>Output Format: The output format will depend on the input data format. If the input data is in conversation format, the output will be in conversation format. If the input data is in OASST format, the output will be in OASST format.</li> </ol>"},{"location":"concepts/multimodal/audio_to_text/","title":"Audio to Text Data Generation","text":"<p>This module introduces support for multimodal data generation pipelines that accept audio or audio + text as input and produce textual outputs using audio-capable LLMs like <code>Qwen2-Audio-7B</code>. It expands traditional text-only pipelines to support audio reasoning tasks like speech recognition, audio classification, and multimodal QA.</p>"},{"location":"concepts/multimodal/audio_to_text/#key-features","title":"Key Features","text":"<ul> <li>Supports audio-only and audio+text prompts.</li> <li>Converts audio fields into base64-encoded data URLs compatible with LLM APIs.</li> <li>Compatible with HuggingFace datasets, streaming, and on-disk formats.</li> <li>Automatically handles lists of audio per field.</li> <li>Seamless round-tripping between loading, prompting, and output publishing.</li> </ul>"},{"location":"concepts/multimodal/audio_to_text/#supported-image-input-types","title":"Supported Image Input Types","text":"<p>Each audio field in a dataset record may be one of the following:</p> <ul> <li>Local file path (e.g., <code>\"data/aud.wav\"</code>)</li> <li>Supported Extensions: <code>.wav</code>, <code>.flac</code>, <code>.ogg</code>, <code>mp3</code>, <code>.m4a</code>, <code>.aac</code>, <code>.aiff</code></li> <li>HTTP(S) URL (e.g., <code>\"https://example.com/audio.wav\"</code>)</li> <li>Raw <code>bytes</code></li> <li>HuggingFace <code>datasets.Audio</code> object</li> <li>Dictionary: <code>{ \"bytes\": &lt;byte_data&gt; }</code></li> <li>A list of any of the above</li> <li>A base64-encoded data URL (e.g., <code>\"data:audio/wav;base64,...\"</code>)</li> </ul>"},{"location":"concepts/multimodal/audio_to_text/#input-source-local-disk-dataset","title":"Input Source: Local Disk Dataset","text":"<p>Supports <code>.json</code>, <code>.jsonl</code>, or <code>.parquet</code> datasets with local or remote audio paths.</p>"},{"location":"concepts/multimodal/audio_to_text/#file-layout","title":"File Layout","text":"<pre><code>project/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 000001.wav\n\u2502   \u251c\u2500\u2500 000002.wav\n\u2502   \u2514\u2500\u2500 input.json\n</code></pre>"},{"location":"concepts/multimodal/audio_to_text/#datainputjson","title":"<code>data/input.json</code>","text":"<pre><code>[\n  { \"id\": \"1\", \"audio\": \"data/000001.wav\" },\n  { \"id\": \"2\", \"audio\": \"https://example.com/audio.wav\" }\n]\n</code></pre>"},{"location":"concepts/multimodal/audio_to_text/#configuration","title":"Configuration","text":"<pre><code>data_config:\n  source:\n    type: \"disk\"\n    file_path: \"data/input.json\"\n</code></pre> <ul> <li>Local paths are resolved relative to <code>file_path</code>.</li> <li>Remote URLs are fetched and encoded to base64 automatically.</li> </ul>"},{"location":"concepts/multimodal/audio_to_text/#input-source-huggingface-dataset","title":"Input Source: HuggingFace Dataset","text":"<p>Supports datasets hosted on the HuggingFace Hub in streaming or download mode.</p>"},{"location":"concepts/multimodal/audio_to_text/#example-record","title":"Example Record","text":"<pre><code>{ \"id\": \"1\", \"audio\": \"HuggingFace datasets.Audio object or URL\" }\n</code></pre>"},{"location":"concepts/multimodal/audio_to_text/#configuration_1","title":"Configuration","text":"<pre><code>data_config:\n  source:\n    type: \"hf\"\n    repo_id: \"myorg/my-dataset\"\n    config_name: \"default\"\n    split: \"train\"\n    streaming: true\n</code></pre> <ul> <li>Handles both <code>datasets.Audio</code> fields and string URLs.</li> <li>Audio is resolved and encoded to base64.</li> </ul>"},{"location":"concepts/multimodal/audio_to_text/#multiple-audio-fields","title":"Multiple Audio Fields","text":"<p>If a record has more than one audio fields (e.g., <code>\"bird_sounds\"</code> and <code>\"animal_sounds\"</code>), reference them individually:</p> <pre><code>- type: audio_url\n  audio_url: \"{bird_sounds}\"\n- type: audio_url\n  audio_url: \"{animal_sounds}\"\n</code></pre>"},{"location":"concepts/multimodal/audio_to_text/#how-audio-transformation-works","title":"How Audio Transformation Works","text":"<ol> <li>Detects audio-like fields from supported types.</li> <li>Converts each to a base64-encoded <code>data:audio/...</code> string.</li> <li> <p>Expands fields containing list of audio internally into multiple prompt entries.</p> <p>Input: <pre><code>{ \"audio\": [\"data/000001.wav\", \"data/000002.wav\"] }\n</code></pre></p> <p>Prompt config:</p> <pre><code>- type: audio_url\n  audio_url: \"{audio}\"\n</code></pre> <p>Will expand to:</p> <p><pre><code>- type: audio_url\n  audio_url: \"data:audio/wav;base64,...\"\n- type: audio_url\n  audio_url: \"data:audio/wav;base64,...\"\n</code></pre> 4. Leaves already-encoded data URLs unchanged.</p> </li> </ol>"},{"location":"concepts/multimodal/audio_to_text/#huggingface-sink-round-tripping","title":"HuggingFace Sink Round-Tripping","text":"<p>When saving output back to HuggingFace datasets:</p> <pre><code>sink:\n  type: \"hf\"\n  repo_id: \"&lt;your_repo&gt;\"\n  config_name: \"&lt;your_config&gt;\"\n  split: \"train\"\n  push_to_hub: true\n  private: true\n  token: \"&lt;hf_token&gt;\"\n</code></pre> <p>Each field that originally contained a <code>data:audio/...</code> base64 string will be: - Decoded back into a HuggingFace datasets.Audio object. - Stored in its native audio format in the output dataset. - Uploaded to the dataset repo as proper audio entries (not strings).</p>"},{"location":"concepts/multimodal/audio_to_text/#example-configuration-identify-the-animal-in-the-audio","title":"Example Configuration: Identify the animal in the audio","text":"<pre><code>data_config:\n  source:\n    type: \"hf\"\n    repo_id: \"datasets-examples/doc-audio-1\"\n    split: \"train\"\n    streaming: true\n\n  sink:\n    type: \"hf\"\n    repo_id: ServiceNow-AI/GraSP\n    config_name: MM-doc-audio-1\n    split: train\n    push_to_hub: true\n    private: true\n    token: \"&lt;hf_token&gt;\"\n\ngraph_config:\n  nodes:\n    identify_animal:\n      output_keys: animal\n      node_type: llm\n      prompt:\n        - user:\n            - type: text\n              text: |\n                Identify the animal in the provided audio.\n            - type: audio_url\n              audio_url: \"{audio}\"\n\n      model:\n        name: qwen_2_audio_7b\n        parameters:\n          max_tokens: 1000\n          temperature: 0.3\n  edges:\n    - from: START\n      to: identify_animal\n    - from: identify_animal\n      to: END\n\noutput_config:\n    output_map:\n        id:\n          from: \"id\"\n        audio:\n          from: \"audio\"\n        animal:\n          from: \"animal\"\n</code></pre>"},{"location":"concepts/multimodal/audio_to_text/#notes","title":"Notes","text":"<ul> <li>Audio generation is not supported in this module. The <code>audio_url</code> type is strictly for passing existing audio inputs (e.g., loaded from datasets), not for generating new audio via model output.</li> <li>For a complete working example, see: <code>tasks/audio_to_text</code></li> </ul>"},{"location":"concepts/multimodal/image_to_text/","title":"Image to Text Data Generation","text":"<p>This module introduces support for multimodal data generation pipelines that accept images or image + text as input and produce textual outputs using vision-capable LLMs like <code>gpt-4o</code>. It expands traditional text-only pipelines to support visual reasoning tasks like chart judgment, document analysis, and multimodal QA.</p>"},{"location":"concepts/multimodal/image_to_text/#key-features","title":"Key Features","text":"<ul> <li>Supports image-only and image+text prompts.</li> <li>Converts image fields into base64-encoded data URLs compatible with LLM APIs.</li> <li>Compatible with HuggingFace datasets, streaming, and on-disk formats.</li> <li>Automatically handles lists of images per field.</li> <li>Seamless round-tripping between loading, prompting, and output publishing.</li> </ul>"},{"location":"concepts/multimodal/image_to_text/#supported-image-input-types","title":"Supported Image Input Types","text":"<p>Each image field in a dataset record may be one of the following:</p> <ul> <li>Local file path (e.g., <code>\"data/img1.png\"</code>)</li> <li>Supported Extensions: <code>.jpg</code>, <code>.jpeg</code>, <code>.png</code>, <code>.gif</code>, <code>.bmp</code>, <code>.tiff</code>, <code>.tif</code>, <code>.webp</code>, <code>.ico</code>, <code>.apng</code></li> <li>HTTP(S) URL (e.g., <code>\"https://example.com/img.png\"</code>)</li> <li>Raw <code>bytes</code></li> <li><code>PIL.Image</code> object</li> <li>Dictionary: <code>{ \"bytes\": &lt;byte_data&gt; }</code></li> <li>A list of any of the above</li> <li>A base64-encoded data URL (e.g., <code>\"data:image/png;base64,...\"</code>)</li> </ul>"},{"location":"concepts/multimodal/image_to_text/#input-source-local-disk-dataset","title":"Input Source: Local Disk Dataset","text":"<p>Supports <code>.json</code>, <code>.jsonl</code>, or <code>.parquet</code> datasets with local or remote image paths.</p>"},{"location":"concepts/multimodal/image_to_text/#file-layout","title":"File Layout","text":"<pre><code>project/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 000001.png\n\u2502   \u251c\u2500\u2500 000002.png\n\u2502   \u2514\u2500\u2500 input.json\n</code></pre>"},{"location":"concepts/multimodal/image_to_text/#datainputjson","title":"<code>data/input.json</code>","text":"<pre><code>[\n  { \"id\": \"1\", \"image\": \"data/000001.png\" },\n  { \"id\": \"2\", \"image\": \"https://example.com/image2.png\" }\n]\n</code></pre>"},{"location":"concepts/multimodal/image_to_text/#configuration","title":"Configuration","text":"<pre><code>data_config:\n  source:\n    type: \"disk\"\n    file_path: \"data/input.json\"\n</code></pre> <ul> <li>Local paths are resolved relative to <code>file_path</code>.</li> <li>Remote URLs are fetched and encoded to base64 automatically.</li> </ul>"},{"location":"concepts/multimodal/image_to_text/#input-source-huggingface-dataset","title":"Input Source: HuggingFace Dataset","text":"<p>Supports datasets hosted on the HuggingFace Hub in streaming or download mode.</p>"},{"location":"concepts/multimodal/image_to_text/#example-record","title":"Example Record","text":"<pre><code>{ \"id\": \"1\", \"image\": \"PIL.Image object or URL\" }\n</code></pre>"},{"location":"concepts/multimodal/image_to_text/#configuration_1","title":"Configuration","text":"<pre><code>data_config:\n  source:\n    type: \"hf\"\n    repo_id: \"myorg/my-dataset\"\n    config_name: \"default\"\n    split: \"train\"\n    streaming: true\n</code></pre> <ul> <li>Handles both <code>datasets.Image</code> fields and string URLs.</li> <li>Images are resolved and encoded to base64.</li> </ul>"},{"location":"concepts/multimodal/image_to_text/#multiple-image-fields","title":"Multiple Image Fields","text":"<p>If a record has more than one image field (e.g., <code>\"chart\"</code> and <code>\"legend\"</code>), reference them individually:</p> <pre><code>- type: image_url\n  image_url: \"{chart}\"\n- type: image_url\n  image_url: \"{legend}\"\n</code></pre>"},{"location":"concepts/multimodal/image_to_text/#how-image-transformation-works","title":"How Image Transformation Works","text":"<ol> <li>Detects image-like fields from supported types.</li> <li>Converts each to a base64-encoded <code>data:image/...</code> string.</li> <li> <p>Expands fields containing list of images internally into multiple prompt entries.</p> <p>Input: <pre><code>{ \"image\": [\"img1.png\", \"img2.png\"] }\n</code></pre></p> <p>Prompt config:</p> <pre><code>- type: image_url\n  image_url: \"{image}\"\n</code></pre> <p>Will expand to:</p> <p><pre><code>- type: image_url\n  image_url: \"data:image/png;base64,...\"\n- type: image_url\n  image_url: \"data:image/png;base64,...\"\n</code></pre> 4. Leaves already-encoded data URLs unchanged.</p> </li> </ol>"},{"location":"concepts/multimodal/image_to_text/#huggingface-sink-round-tripping","title":"HuggingFace Sink Round-Tripping","text":"<p>When saving output back to HuggingFace datasets:</p> <pre><code>sink:\n  type: \"hf\"\n  repo_id: \"&lt;your_repo&gt;\"\n  config_name: \"&lt;your_config&gt;\"\n  split: \"train\"\n  push_to_hub: true\n  private: true\n  token: \"&lt;hf_token&gt;\"\n</code></pre> <p>Each field that originally contained a <code>data:image/...</code> base64 string will be: - Decoded back into a PIL Image. - Stored in its native image format in the output dataset. - Uploaded to the dataset repo as proper image entries (not strings).</p>"},{"location":"concepts/multimodal/image_to_text/#example-configuration-graph-quality-judgement","title":"Example Configuration: Graph Quality Judgement","text":"<pre><code>data_config:\n  source:\n    type: \"hf\"\n    repo_id: \"&lt;repo_id&gt;\"\n    config_name: \"&lt;config_name&gt;\"\n    split: \"train\"\n    streaming: true\n    transformations:\n      - transform: grasp.processors.data_transform.AddNewFieldTransform\n        params:\n          mapping:\n            graph_judgement: \"\"\n            graph_judgement_content: \"\"\n\ngraph_config:\n  nodes:\n    judge_synthetic_graph_quality:\n      node_type: llm\n      post_process: tasks.image_description.task_executor.GraphJudgementPostProcessor\n      prompt:\n        - user:\n            - type: text\n              text: |\n                You are given a graph image that represents structured numerical data.\n                ...\n                Output Format:\n                &lt;JUDGEMENT&gt;\n                accept/reject\n                &lt;/JUDGEMENT&gt;\n                &lt;JUDGEMENT_EXPLANATION&gt;\n                Explanation goes here.\n                &lt;/JUDGEMENT_EXPLANATION&gt;\n            - type: image_url\n              image_url: \"{image}\"\n      model:\n        name: gpt-4o\n        parameters:\n          max_tokens: 1000\n          temperature: 0.3\n\n  edges:\n    - from: START\n      to: judge_synthetic_graph_quality\n    - from: judge_synthetic_graph_quality\n      to: END\n\noutput_config:\n  output_map:\n    id: \n      from: \"id\"\n    image: \n      from: \"image\"\n    graph_judgement: \n      from: \"graph_judgement\"\n    graph_judgement_content: \n      from: \"graph_judgement_content\"\n</code></pre>"},{"location":"concepts/multimodal/image_to_text/#notes","title":"Notes","text":"<ul> <li>Image generation is not supported in this module. The <code>image_url</code> type is strictly for passing existing image inputs (e.g., loaded from datasets), not for generating new images via model output.</li> <li>For a complete working example, see: <code>tasks/image_to_qna</code></li> </ul>"},{"location":"concepts/nodes/agent_node/","title":"Agent Node","text":""},{"location":"concepts/nodes/agent_node/#agent-node","title":"Agent Node","text":"<p>GraSP supports creating and executing agents using the ReAct (Reasoning and Acting) framework. Agent nodes extend LLM nodes with additional capabilities for tool usage, reasoning, and autonomous decision-making.</p> <p>To use it, include the following configuration in your <code>graph_config.yaml</code> file:</p>"},{"location":"concepts/nodes/agent_node/#example-configuration","title":"Example Configuration:","text":"<pre><code>research_agent:\n  node_type: agent\n  prompt:\n    - system: |\n        You are a research assistant that helps users find information. Use the provided tools to search for information and answer the user's question.\n        Always think step by step and explain your reasoning.\n    - user: |\n        Please help me research {topic}. I'm particularly interested in {specific_aspect}.\n  tools:\n    - tasks.agent_tool_simulation.tools.search_tool.search\n    - tasks.agent_tool_simulation.tools.calculator_tool.calculate\n  inject_system_messages:\n    2: \"Remember to cite your sources when providing information.\"\n    4: \"If you're unsure about something, acknowledge the uncertainty rather than making up information.\"\n  output_keys:\n    - agent_response\n  model:\n    name: vllm_model\n    parameters:\n      temperature: 0.2\n      max_tokens: 1024\n</code></pre>"},{"location":"concepts/nodes/agent_node/#configuration-fields","title":"Configuration Fields:","text":"<ul> <li> <p><code>node_type</code>: This should be set to <code>agent</code>.</p> </li> <li> <p><code>prompt</code>: This is the prompt that will be sent to the agent. The prompt implementation is same as any other node with system, user and assistant turn, however system turn must be added to define agent behavior.</p> </li> <li> <p><code>tools</code>: A list of tools that the agent can use. Tools are provided to the agent through the <code>create_react_agent</code> function from LangGraph.   The following tools are currently supported:</p> </li> <li><code>tasks.examples.agent_tool_simulation.tools_from_module</code> All valid tools from a module.</li> <li><code>tasks.examples.agent_tool_simulation.tools_from_class</code> All valid tools from a class.</li> </ul> <p>Make sure all the necessary tools are decorated with <code>@tool</code> from <code>langchain_core.tools</code></p> <ul> <li> <p><code>inject_system_messages</code>: Optional dictionary where keys are conversation turn indices and values are additional system messages to inject at those turns. This allows for dynamic guidance of the agent based on conversation length.</p> </li> <li> <p><code>post_process</code>: This is the function class of <code>type NodePostProcessor</code>, used to post-process the output from the agent.   The class needs to define an <code>apply()</code> method with parameter <code>GraspMessage</code>. This allows for extracting specific information from the agent's response.</p> </li> <li> <p><code>output_keys</code>: These are the variables used to store the output from the agent. This can be a list or a single variable.   If a postprocessor is defined, <code>output_keys</code> can include multiple variables.</p> </li> <li> <p><code>model</code>: This defines the model to be used for the agent. The model must be compatible with the BaseChatModel interface for proper agent functionality.</p> </li> <li>The ModelFactory will automatically select the appropriate model implementation based on the model type.</li> <li> <p>NOTE: </p> <ul> <li>The model extending from BaseChatModel should be implemented in models/langgraph folder. Refer to <code>CustomVLLMChatModel</code> in <code>models/langgraph/vllm_chat_model.py</code></li> <li><code>backend: langgraph</code> Needs be set in models.yaml against the model being used in agent node.</li> <li>Supported model types: <code>vllm, azure_openai</code></li> </ul> </li> <li> <p><code>pre_process</code>: This is an optional functional class of type <code>NodePreProcessor</code>, used to preprocess the input before sending it to the agent.</p> </li> <li> <p><code>input_key</code>: This is an optional field to specify the input key for the agent node. If not defined, the default input   key (<code>messages</code>) will be used.</p> </li> <li> <p><code>chat_history</code>: Boolean flag to enable or disable chat history for multi-turn conversations. When enabled, the agent can reference previous interactions.</p> </li> </ul>"},{"location":"concepts/nodes/agent_node/#inherited-features-from-llm-node","title":"Inherited Features from LLM Node:","text":"<p>Agent nodes inherit all the features of LLM nodes, including:</p> <ul> <li>Support for chat history</li> <li>Input/output key configuration</li> <li>Pre/post processing capabilities</li> <li>Output role specification</li> </ul>"},{"location":"concepts/nodes/agent_node/#key-differences-from-llm-node","title":"Key Differences from LLM Node:","text":"<ol> <li> <p>Tool Usage: Agent nodes can use tools to interact with external systems or perform specific functions.</p> </li> <li> <p>ReAct Framework: Agent nodes use the ReAct framework from LangGraph, which enables reasoning and acting cycles.</p> </li> <li> <p>Model Requirements: Agent nodes require models that implement the BaseChatModel interface.</p> </li> <li> <p>System Message Injection: Agent nodes support dynamic injection of system messages based on conversation turn.</p> </li> </ol>"},{"location":"concepts/nodes/agent_node/#implementation-details","title":"Implementation Details:","text":"<p>The agent node uses LangGraph's <code>create_react_agent</code> function to create a ReAct agent with the specified model and tools. The agent's system prompt is composed of the base prompt and any injected system messages based on the conversation turn.</p> <p>When executed, the agent goes through reasoning and acting cycles until it reaches a final answer, which is then post-processed and returned as specified in the <code>output_keys</code>.</p>"},{"location":"concepts/nodes/lambda_node/","title":"Lambda Node","text":""},{"location":"concepts/nodes/lambda_node/#lambda-node","title":"Lambda Node","text":"<p>GraSP supports custom logic in your workflow using the lambda node. Lambda nodes allow you to execute arbitrary Python functions or classes, making them ideal for custom data processing, state manipulation, or integration of unique logic that doesn't fit standard node types.</p>"},{"location":"concepts/nodes/lambda_node/#example-configuration","title":"Example Configuration","text":"<pre><code>lambda_function_node:\n  node_type: lambda\n  lambda: path.to.module.function_name or path.to.module.LambdaFunctionImplementationClass\n  output_keys: \n    - return_key1\n    - return_key2 \n</code></pre>"},{"location":"concepts/nodes/lambda_node/#configuration-fields","title":"Configuration Fields","text":"<ul> <li> <p><code>node_type</code>:   Set to <code>lambda</code> to indicate this node type.</p> </li> <li> <p><code>lambda</code>:   Fully qualified path to the function or class to execute.  </p> </li> <li>Can be a direct function (e.g., <code>tasks.my_task.task_executor.lambda_function</code>)</li> <li> <p>Or a class that implements the <code>LambdaFunction</code> interface (e.g., <code>tasks.my_task.task_executor.TestLambda</code>)</p> </li> <li> <p><code>output_keys</code>:   List of keys from the return dictionary or state that will be made available to subsequent nodes.</p> </li> <li> <p><code>node_state</code>:   Optional. Node-specific state key.</p> </li> </ul>"},{"location":"concepts/nodes/lambda_node/#example-lambda-implementation","title":"Example Lambda Implementation","text":"<p>You can implement a lambda either as a class or a function:</p> <pre><code># Example in yaml: lambda: tasks.my_task.task_executor.TestLambda\nfrom grasp.core.graph.functions.lambda_function import LambdaFunction\nfrom grasp.core.graph.grasp_state import GraspState\n\nclass TestLambda(LambdaFunction):\n    def apply(lambda_node_dict: dict, state: GraspState):\n        state[\"return_key1\"] = \"hello world\"\n        state[\"return_key2\"] = \"dummy world\"\n        return state\n\n# Or as a direct function:\ndef lambda_function(lambda_node_dict: dict, state: GraspState):\n    state[\"return_key1\"] = \"hello world\"\n    state[\"return_key2\"] = \"dummy world\"\n    return state\n</code></pre>"},{"location":"concepts/nodes/lambda_node/#notes","title":"Notes","text":"<ul> <li>Lambda nodes give you full control over data transformation, allowing you to bridge, preprocess, or postprocess state as needed.</li> <li>All keys you want accessible in the next node should be listed in <code>output_keys</code>.</li> <li>Use lambda nodes for any custom task, especially when built-in nodes do not cover your use case.</li> </ul> <p>Tip: Keep your lambda logic modular and reusable across tasks for maximum flexibility.</p>"},{"location":"concepts/nodes/llm_node/","title":"LLM Node","text":""},{"location":"concepts/nodes/llm_node/#llm-node","title":"LLM Node","text":"<p>GraSP supports text generation using LLMs. GraSP provides integration with various LLMs hosted on different inference servers.</p> <p>To use it, include the following configuration in your <code>graph_config.yaml</code> file:</p>"},{"location":"concepts/nodes/llm_node/#example-configuration","title":"Example Configuration:","text":"<pre><code>paraphrase_question:\n  node_type: llm\n  prompt:\n    - system: |\n        You are an assistant tasked with paraphrasing a user query in a {tone} tone acting as a {persona}. Do NOT change/paraphrase the python code and keep it as is. Do NOT generate any conversational text and respond ONLY with the paraphrased query in the following format: \"PARAPHRASED QUERY: &lt;query&gt;\"\n    - user: |\n        USER QUERY: Provide a brief description of the problem the code is trying to solve and a brief explanation of the code. Do NOT generate any conversational text and respond ONLY with the problem the code is trying to solve and the explanation of the code.\n\n        {code}\n  post_process: tasks.mbpp.code_explanation.task_executor.ParaphraseQuestionNodePostProcessor\n  output_keys:\n    - rephrased_text\n  model:\n    name: mistralai\n    parameters:\n      temperature: 0.3\n</code></pre>"},{"location":"concepts/nodes/llm_node/#configuration-fields","title":"Configuration Fields:","text":"<ul> <li> <p><code>node_type</code>: This should be set to <code>llm</code>.</p> </li> <li> <p><code>prompt</code>: This is the prompt that will be sent to the LLM. It should contain the system prompt and the user prompt.   The system prompt defines the instructions for the LLM, and the user prompt provides the user query.</p> </li> <li> <p><code>post_process</code>: This is the function class of <code>type NodePostProcessor</code>, used to post-process the output from the LLM.    The class need to define <code>apply()</code> method with parameter <code>GraspMessage</code>. <code>GraspMessage</code> is just a wrapper on the actual LangGraph message object(<code>AIMessage</code>, <code>UserMessage</code>, etc).   Please note, if the variables returned by the above method are required as state variables, they should be defined in the <code>output_vars</code> field for the node.   This also has backward compatibility, you can set a direct method to <code>post_process</code> with the above signature.</p> </li> <li> <p><code>output_keys</code>: These are the variables used to store the output from the LLM. This is typically a method defined by   the user in their <code>task_executor</code> file. It can be a list or a single variable.</p> <ul> <li>If a postprocessor is not defined, the default postprocessor is invoked, and the output is stored in   <code>output_keys</code>.</li> <li>If a postprocessor is defined, <code>output_keys</code> can include multiple variables.</li> <li>Note: <code>output_vars</code> and <code>output_key</code> are deprecated.</li> <li>Note:  With this change, access the output_keys directly from state variable.</li> <li>Note: By default, the returned message is an assistant message. To change the role of the message, use   <code>output_role</code>.</li> </ul> </li> <li> <p><code>model</code>: This defines the LLM model to be used. The primary model configuration should be specified in the   <code>models.yaml</code> file under the config folder. Parameters defined in the node override those in <code>models.yaml</code>.</p> </li> <li> <p><code>pre_process</code>: This is an optional functional class of type <code>NodePreProcessor</code>, used to preprocess the input before sending it to the LLM. If not   provided, the default preprocessor is used. This class need to define <code>apply</code> method with <code>GraspState</code> as a parameter.</p> </li> </ul> <p>Example code:   ```python   class CritiqueAnsNodePreProcessor(NodePreProcessor):       def apply(self, state:GraspState):           if not state[\"messages\"]:               state[\"messages\"] = []</p> <pre><code>      # We need to convert user turns to assistant and vice versa\n      cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n      translated = [cls_map[msg.type](content=msg.content) for msg in state[\"messages\"]]\n      state.update({\"messages\": translated})\n      return state\n</code></pre> <p><code>``   It also supports backward compatibility, user can set a simple method into</code>pre_process`.</p> <ul> <li> <p><code>output_key</code>: The old behavior is still maintained with <code>output_keys</code>. But this variable is renamed, this may impact graph_config.yaml file and the output generator code.</p> </li> <li> <p><code>input_key</code>: This is an optional field to specify the input key for the LLM node. If not defined, the default input   key (<code>messages</code>) will be used.</p> </li> <li> <p><code>output_role</code>: This defines the role of the message returned by the LLM. It can be <code>system</code>, <code>user</code>, or <code>assistant</code>.   If not specified, the default role (<code>assistant</code>) will be used.</p> </li> </ul>"},{"location":"concepts/nodes/multi_llm_node/","title":"Multi LLM Node","text":""},{"location":"concepts/nodes/multi_llm_node/#multi-llm-node","title":"Multi-LLM Node","text":"<p>GraSP supports the ability to run multiple large language models in parallel using the multi_llm node. This is useful for benchmarking, model comparison, or workflows where you want to generate multiple outputs for the same prompt and aggregate or rate the results.</p> <p>A multi-llm node sends the same prompt to several configured models and collects all responses together in the state.</p>"},{"location":"concepts/nodes/multi_llm_node/#example-configuration","title":"Example Configuration","text":"<pre><code>generate_samples:  \n  node_type: multi_llm\n  pre_process: tasks.dpo_samples.task_executor.generate_samples_pre_process\n  output_keys: model_responses    # Optional: key to store all model responses in state\n  prompt:        \n    - user: \"{user_prompt}\"\n  models:      \n    mistral_large:               # Model identifier\n      name: mistralai            # Model name (must be defined in models.yaml)\n      parameters:          \n        max_tokens: 2000\n        temperature: 0.7            \n    mixtral_instruct_8x22b:         # Model identifier\n      name: mixtral_instruct_8x22b  # Model name (must be defined in models.yaml)\n      parameters:\n        temperature: 0.7\n        max_tokens: 2000\n  multi_llm_post_process: tasks.dpo_samples.task_executor.post_process_responses  # Optional\n</code></pre>"},{"location":"concepts/nodes/multi_llm_node/#configuration-fields","title":"Configuration Fields","text":"<ul> <li> <p><code>node_type</code>:   Set to <code>multi_llm</code> for this node type.</p> </li> <li> <p><code>prompt</code>:   The prompt/messages to send to each model. Supports templated variables (e.g., <code>{user_prompt}</code>).</p> </li> <li> <p><code>models</code>:   Map of model identifiers to their configuration. Each entry must specify:</p> </li> <li><code>name</code>: The model name as defined in your <code>models.yaml</code>.</li> <li><code>parameters</code>: Model-specific parameters (such as <code>temperature</code>, <code>max_tokens</code>, etc).</li> <li> <p>Each model receives the same prompt, but you can specify different parameters for each.</p> </li> <li> <p><code>output_keys</code>:   The key in the state where the multi-model responses are stored (default: <code>\"messages\"</code>).</p> </li> <li> <p><code>pre_process</code>:   Optional. Path to a function or class (with an <code>apply()</code> method) to transform the input before calling the models.</p> </li> <li> <p><code>multi_llm_post_process</code>:   Optional. Path to a function or class (with an <code>apply()</code> method) to process the collected responses after calling all models. By default, responses are stored as a dictionary with model names as keys.</p> </li> <li> <p><code>input_key</code>:   Optional. State key to read input messages from. Defaults to <code>\"messages\"</code>.</p> </li> <li> <p><code>node_state</code>:   Optional. Node-specific state key.</p> </li> </ul>"},{"location":"concepts/nodes/multi_llm_node/#output","title":"Output","text":"<p>The output from a multi-llm node will contain a mapping of model names to their respective responses, for example:</p> <pre><code>{\n  \"model_responses\": {\n    \"gpt4\": { \"message\": \"...\", \"success\": true },\n    \"gpt-4o\": { \"message\": \"...\", \"success\": true },\n    \"gpt4o-mini\": \"A sustainable urban transportation system should focus on...\"\n  }\n}\n</code></pre>"},{"location":"concepts/nodes/multi_llm_node/#reference-example-dpo-samples-task","title":"Reference Example: DPO Samples Task","text":"<p>For a full workflow demonstrating the use of the multi-llm node\u2014including advanced structured output, post-processing, and integration with judge models\u2014see the DPO Samples Task example in this documentation.</p>"},{"location":"concepts/nodes/multi_llm_node/#notes","title":"Notes","text":"<ul> <li>Model Responses: All configured models are called in parallel with the same prompt, and their outputs are collected together.</li> <li>Custom Processing: Use <code>pre_process</code> and <code>multi_llm_post_process</code> to customize how inputs and outputs are handled.</li> <li>Flexible Output: Each model\u2019s structured output can be configured independently using YAML schema or class-based definitions. See the referenced DPO example for details.</li> <li>Use Cases: Useful for data generation, preference optimization, or any scenario where model comparison or diversity is required.</li> </ul> <p>Tip: Combine multi-llm nodes with judge models and edge conditions for advanced workflows such as DPO (Direct Preference Optimization), rating, or filtering.</p>"},{"location":"concepts/nodes/subgraph_node/","title":"Subgraph Node","text":""},{"location":"concepts/nodes/subgraph_node/#subgraph-node","title":"Subgraph Node","text":"<p>GraSP supports modular graph design through the subgraph node. Subgraphs allow you to encapsulate and reuse a sequence of nodes and edges as a single unit within a main graph. This makes complex workflows more organized, reusable, and easier to maintain.</p> <p>You can include a subgraph in your main graph and customize the configuration of its internal nodes as needed for your specific workflow.</p>"},{"location":"concepts/nodes/subgraph_node/#example-configuration","title":"Example Configuration","text":""},{"location":"concepts/nodes/subgraph_node/#subgraph-definition-generate_question-subgraph","title":"Subgraph Definition (<code>generate_question</code> subgraph)","text":"<pre><code># Example: tasks/subgraph_examples/subgraphs/generate_question.yaml\ngraph_config:\n  nodes:\n    persona_sampler:\n      node_type: weighted_sampler\n      attributes:\n        num_turns:\n          values: [2, 3, 4, 5]\n        tone1:\n          values: [professional, casual, friendly]\n        persona1:\n          values: [high school teacher, software engineer]\n\n    paraphrase_question:\n      node_type: llm\n      output_keys: text\n      prompt:\n        - system: |\n            Assume you are {persona1} persona.\n            Paraphrase the question while preserving all details.\n        - user: |\n            QUESTION: {p}\n      model:\n        name: gpt4o\n        parameters:\n          temperature: 0.1\n\n  edges:\n    - from: START\n      to: persona_sampler\n    - from: persona_sampler\n      to: paraphrase_question\n    - from: paraphrase_question\n      to: END\n</code></pre>"},{"location":"concepts/nodes/subgraph_node/#main-graph-including-the-subgraph","title":"Main Graph Including the Subgraph","text":"<pre><code>nodes:\n  generate_question:\n    node_type: subgraph\n    subgraph: tasks.subgraph_examples.subgraphs.generate_question\n    node_config_map:\n      paraphrase_question:\n        output_keys: rephrased_text\n        prompt_placeholder_map:\n          p: prompt\n        model:\n          name: gpt4o\n          parameters:\n            temperature: 1.0\n</code></pre>"},{"location":"concepts/nodes/subgraph_node/#configuration-fields","title":"Configuration Fields","text":"<ul> <li> <p><code>node_type</code>:   Should be set to <code>subgraph</code> to indicate this node is a subgraph reference.</p> </li> <li> <p><code>subgraph</code>:   The import path or module reference to the subgraph definition. This should point to the graph configuration file that defines the subgraph.</p> </li> <li> <p><code>node_config_map</code>(Optional) :   Dictionary to customize nodes inside the subgraph for this usage.</p> </li> <li>The keys are node names within the subgraph (e.g., <code>paraphrase_question</code>).</li> <li>The values are configuration fields you wish to modify for that specific subgraph node, such as <code>output_keys</code>, <code>model</code>, <code>prompt</code>, etc.</li> <li> <p>Note: Only the fields explicitly provided in <code>node_config_map</code> are overridden for the specified subgraph node. All other settings for that node remain as defined in the original subgraph.</p> <p>Supported fields include (but are not limited to): - <code>output_keys</code>: Override the output keys for the subgraph node. - <code>prompt_placeholder_map</code>: Remap input placeholders in prompts. For example, if the subgraph uses <code>{p}</code> but your main graph uses <code>prompt</code>, you can set <code>prompt_placeholder_map: { p: prompt }</code>. - <code>model</code>: Override model name or parameters. - Any other valid node configuration field.</p> </li> </ul>"},{"location":"concepts/nodes/subgraph_node/#example-using-prompt_placeholder_map","title":"Example: Using <code>prompt_placeholder_map</code>","text":"<p>If a subgraph node\u2019s prompt uses <code>p</code> but your main graph contains <code>prompt</code> as input state variable, you can map this as follows:</p> <p><pre><code>prompt_placeholder_map:\n  p: prompt\n</code></pre> This will update all occurrences of <code>{p}</code> in the subgraph prompt of that node to <code>{prompt}</code>.</p>"},{"location":"concepts/nodes/subgraph_node/#notes","title":"Notes","text":"<ul> <li>Customization: With <code>node_config_map</code>, you can adapt subgraph nodes to fit your main graph\u2019s needs\u2014change output keys, update model parameters, or map input variables as needed. Only the fields you specify are changed; all others remain as originally set in the subgraph.</li> <li>Prompt Placeholder Mapping: Use <code>prompt_placeholder_map</code> to easily align variable names between your main graph and subgraph's prompt, without changing the original subgraph.</li> <li>Reusability: Subgraphs help you avoid duplication by making it easy to reuse common sequences or workflows across multiple graphs.</li> <li>Compatibility: Adding subgraph nodes does not affect existing graphs that don\u2019t use this feature.</li> </ul> <p>Tip: Use subgraphs to modularize common tasks such as data preprocessing, prompt engineering, or multi-step LLM workflows, and include them in different main graphs with custom settings as needed.</p>"},{"location":"concepts/nodes/weighted_sampler_node/","title":"Weighted Sampler Node","text":""},{"location":"concepts/nodes/weighted_sampler_node/#weighted-sampler-node","title":"Weighted Sampler Node","text":"<p>GraSP supports random selection of values for attributes in your workflow using the weighted_sampler node. This node enables you to sample configuration values\u2014optionally with custom weights\u2014or pull data directly from external sources.</p> <p>Weighted sampler nodes are useful for randomized data generation, prompt diversity, scenario coverage, and more.</p>"},{"location":"concepts/nodes/weighted_sampler_node/#example-configuration","title":"Example Configuration","text":"<pre><code>persona_sampler:\n  node_type: weighted_sampler\n  attributes:\n    num_turns:                # Attribute name\n      values: [2, 3, 4, 5]    # Possible values\n    tone:\n      values: [professional, casual, friendly, formal]\n      weights: [2, 1, 1, 1]   # Optional weights (default: equal weights)\n    question_role:\n      values:\n        column: \"persona\"\n        source:\n          type: \"hf\"\n          repo_id: \"nvidia/Nemotron-Personas\"\n          config_name: \"default\"\n          split: \"train\"\n</code></pre>"},{"location":"concepts/nodes/weighted_sampler_node/#configuration-fields","title":"Configuration Fields","text":"<ul> <li> <p><code>node_type</code>:   Set to <code>weighted_sampler</code> to indicate this node type.</p> </li> <li> <p><code>attributes</code>:   Map of attribute names to their sampling configuration.</p> </li> <li> <p><code>values</code>:     List of possible values to sample from, or configuration for external data sources.  </p> <ul> <li>If a static list is provided, one value is sampled per run.</li> <li>If pointing to a data source (like Huggingface or Disk), values are read sequentially from the source. If the source has fewer records than needed, it loops back to the beginning (non-stream sources only).</li> <li>The <code>column</code> can be a single column name or a list of columns (for column-wise sampling).</li> </ul> </li> <li> <p><code>weights</code> (optional):     List of weights (same length as <code>values</code>) for weighted random sampling. If not specified, all values are sampled with equal probability.</p> </li> </ul>"},{"location":"concepts/nodes/weighted_sampler_node/#advanced-data-source-sampling","title":"Advanced Data Source Sampling","text":"<ul> <li>Static List: <pre><code>num_turns:\n  values: [2, 3, 4, 5]\n</code></pre></li> <li>Data Source (e.g. Huggingface): <pre><code>question_role:\n  values:\n    column: \"persona\"\n    source:\n      type: \"hf\"\n      repo_id: \"nvidia/Nemotron-Personas\"\n      config_name: \"default\"\n      split: \"train\"\n</code></pre></li> <li>Multiple Columns:   If <code>column</code> is a list, one column is picked at random for each record.</li> </ul>"},{"location":"concepts/nodes/weighted_sampler_node/#notes","title":"Notes","text":"<ul> <li>Attribute values can be sampled randomly or sequentially from lists, data files, or Huggingface datasets.</li> <li>When using a non-stream data source and records run out, sampling loops back to the beginning.</li> <li>For best performance, avoid using stream data sources with samplers. Also make sure the dataset is not too large, as it downloads and pick sample in local process.</li> <li>The sampled values are stored as state variables, ready for use by subsequent nodes.</li> </ul> <p>Tip: Use weighted samplers to inject diversity and controlled randomness into your pipeline, or to generate data for experiments and training.</p>"},{"location":"concepts/processors/output_record_generator/","title":"Output Record Generator","text":""},{"location":"concepts/processors/output_record_generator/#output-record-generator","title":"Output Record Generator","text":"<p>The Output Record Generator is a flexible component that translates the final state of the graph into the desired output format for each record. </p>"},{"location":"concepts/processors/output_record_generator/#key-concept","title":"Key Concept:","text":"<p>Configure the final record fields in a YAML block (called <code>output_map</code>), telling the generator how to map fields from the graph's state or use static values\u2014and optionally apply transforms.</p>"},{"location":"concepts/processors/output_record_generator/#usage","title":"Usage:","text":"<p>In the <code>graph_config.yaml</code>, declare an <code>output_config</code> section specifying a <code>generator</code>, such as <code>processors.output_record_generator.CodeGenOutputGenerator</code></p>"},{"location":"concepts/processors/output_record_generator/#1-simple-derived-class","title":"1. Simple Derived Class","text":"<p>Subclass the <code>BaseOutputGenerator</code> and implement the <code>generate()</code> method that returns the final record from the state. </p> <p>Example:</p> <ul> <li> <p>YAML Configuration <pre><code>output_config:\n  generator: grasp.processors.output_record_generator.CodeGenOutputGenerator\n</code></pre></p> </li> <li> <p>Python Code <pre><code>class CodeGenOutputGenerator(BaseOutputGenerator):\n    def generate(self, state: GraphState) -&gt; dict[str, Any]:\n      if \"messages\" not in state:\n          return None\n\n      chat_format_messages = utils.convert_messages_from_langchain_to_chat_format(\n          state[\"messages\"]\n      )\n      if (\n          len(chat_format_messages) &lt; 1\n          or \"no more feedback\"\n          not in chat_format_messages[-1][\"content\"].lower().strip()\n      ):\n          return None\n      # remove the last message if it contains \"no more feedback\"\n      chat_format_messages = chat_format_messages[:-1]\n      chat_format_messages.insert(\n          0,\n          {\n              \"role\": \"user\",\n              \"content\": state[\"rephrased_text\"].replace(\n                  \"PARAPHRASED QUESTION: \", \"\"\n              ),\n          },\n      )\n      return {\n          \"id\": state.get(\"id\", \"\"),\n          \"conversation\": chat_format_messages,\n          \"taxonomy\": [{\"category\": \"Coding\", \"subcategory\": \"\"}],\n          \"annotation_type\": [\"mistral-large\"],\n          \"language\": [\"en\"],\n          \"tags\": [\"mbpp\", \"reannotate\", \"self-critique\"],\n      }\n</code></pre></p> </li> </ul>"},{"location":"concepts/processors/output_record_generator/#2-yaml-driven-mapping-transform","title":"2. YAML driven Mapping &amp; Transform","text":"<p>In this approach, we define a YAML configuration that maps fields from the graph's state or uses static values\u2014and optionally apply transforms.</p> <p>Example:</p> <ul> <li>YAML Configuration</li> </ul> <pre><code>output_config:\n  generator: grasp.processors.output_record_generator.CodeGenOutputGenerator\n\n  output_map:\n    id:\n      from: \"id\"  # Copy from state[\"id\"]\n    conversation:\n      from: \"messages\"\n      transform: \"build_conversation\"  # Apply a transform method on state[\"messages\"]\n    taxonomy:\n      value:\n        - category: \"Coding\"  # Hard-coded literal value\n          subcategory: \"\"\n    annotation_type:\n      value: [ \"mistral-large\" ]   # Hard-coded literal list\n    language:\n      value: \"en\"   # Hard-coded literal value\n    tags:\n      value: [\"mbpp\", \"reannotate\", \"self-critique\"]   # Hard-coded literal list\n</code></pre> <ul> <li> <p>Python Code <pre><code>class CodeGenOutputGenerator(BaseOutputGenerator):\n    \"\"\"\n    Example specialized generator class, which defines a transform method\n    for building a conversation from messages, removing 'no more feedback', etc.\n    \"\"\"\n\n    @staticmethod\n    def build_conversation(data: Any, state: dict[str, Any]) -&gt; Any:\n        chat_format_messages = utils.convert_messages_from_langchain_to_chat_format(data)\n\n        # Example logic:\n        if (\n            not chat_format_messages\n            or \"no more feedback\" not in chat_format_messages[-1][\"content\"].lower().strip()\n        ):\n            return None\n\n        # remove the last message\n        chat_format_messages.pop()\n\n        # insert rephrased question if available\n        if \"rephrased_text\" in state and state[\"rephrased_text\"]:\n            question = state[\"rephrased_text\"].replace(\"PARAPHRASED QUESTION: \", \"\")\n            chat_format_messages.insert(0, {\"role\": \"user\", \"content\": question})\n\n        return chat_format_messages\n</code></pre></p> </li> <li> <p><code>generator</code>: The dotted Python path to the output generator class (e.g., <code>CodeGenOutputGenerator</code>).</p> </li> <li><code>output_map</code>: Each key (e.g., id, conversation, tags) will become a field in the final record.</li> <li><code>from</code>: Means \u201cread from the in-memory <code>state</code> with that key.\u201d</li> <li><code>value</code>: Means \u201cuse this literal value.\u201d</li> <li><code>transform</code>: Means \u201ccall a method on the generator class to transform the data.\u201d This is optional.</li> </ul> <p>Note: If we include <code>\"transform\": \"my_custom_logic\"</code> in the YAML, you then a method named <code>my_custom_logic</code> must be defined in the generator class. </p> <p>Here, we define a <code>build_conversation</code> method that takes the <code>data</code> and <code>state</code> as arguments and returns the transformed conversation.</p>"},{"location":"concepts/schema_validator/","title":"Schema Validator","text":""},{"location":"concepts/schema_validator/#introduction","title":"Introduction","text":"<p>Schema validator enables users to ensure correctness of generated data before uploading to HF or File System.</p> <p>Key features supported for schema validation are as follows:- </p> <ol> <li>YAML based schema check:- Users can define their schema using YAML config files in the following ways:-</li> <li>Define a custom schema class inside <code>custom_schemas.py</code> and add it's path in <code>schema</code> key inside <code>schema_config</code>.</li> <li> <p>Add expected schema config in a list of dict format inside <code>fields</code> key inside <code>schema_config</code>.</p> </li> <li> <p>Rule based validation support:- Aside from adding validator rules inside custom class, users can choose from validation methods supported(details in additional validation rules section) and add it as a key for a particular field's dict.</p> </li> </ol>"},{"location":"concepts/schema_validator/#usage-illustration","title":"Usage Illustration","text":"<p>Let's assume we have the following record generated which we want to validate:- </p> <p><pre><code>{\n        \"id\": 130426,\n        \"conversation\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"I am trying to get the CPU cycles at a specific point in my code.\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"The `rdtsc` function you're using gives you the number of cycles since the CPU was last reset, which is not what you want in this case.\"\n            }\n        ],\n        \"taxonomy\": [\n            {\n                \"category\": \"Coding\",\n                \"subcategory\": \"\"\n            }\n        ],\n        \"annotation_type\": [\n            \"mistral-large\"\n        ],\n        \"language\": [\n            \"en\"\n        ],\n        \"tags\": [\n            \"glaiveai/glaive-code-assistant-v2\",\n            \"reannotate\",\n            \"self-critique\"\n        ]\n}\n</code></pre> For the above record, user can have the following class defined inside <code>custom_schemas.py</code> defining the  expected keys and values along with additional validation rules if any. </p> <pre><code>class CustomUserSchema(BaseModel):\n    '''\n    This demonstrates an example of a customizable user schema that can be modified or redefined by the end user.\n    Below is a sample schema with associated validator methods.\n    '''\n    id: int\n    conversation: list[dict[str,Any]]\n    taxonomy: list[dict[str, Any]]\n    annotation_type: list[str]\n    language: list[str]\n    tags: list[str]\n\n    @root_validator(pre=True)\n    def check_non_empty_lists(cls, values):\n        if not values.get('id'):\n            raise ValueError('id cannot be empty')\n        return values\n</code></pre>"},{"location":"concepts/schema_validator/#sample-yaml-configuration-to-use-custom-schema-defined-in-custom_schemaspy-","title":"Sample YAML configuration to use custom schema defined in custom_schemas.py:-","text":"<pre><code>schema_config:\n  schema: grasp.validators.custom_schemas.CustomUserSchema\n</code></pre>"},{"location":"concepts/schema_validator/#sample-yaml-configuration-to-define-schema-in-yaml-","title":"Sample YAML configuration to define schema in YAML:-","text":"<p><pre><code>schema_config:\n  fields:\n    - name: id\n      type: int\n      is_greater_than: 99999\n    - name: conversation\n      type: list[dict[str, any]]\n    - name: taxonomy\n      type: list[dict[str, any]]\n    - name: annotation_type\n      type: list[str]\n    - name: language\n      type: list[str]\n    - name: tags\n      type: list[str]\n</code></pre> Note that <code>fields</code> is expected to be a list of dicts with <code>name</code> and <code>type</code> present in each dict with additional option of providing validation key. In the above example <code>is_greater_than</code> is a validation key shown for demonstration purpose  to ensure <code>id</code> key in each record has a value with 6 digits or more. </p>"},{"location":"concepts/schema_validator/#additional-validation-rules-supported-","title":"Additional Validation Rules Supported:-","text":"<p>Currently we support the following validation rules that can be directly used by the user:- </p> <ol> <li><code>is_greater_than</code>: Ensures value present in a given field is greater than value provided by user in schema definition. </li> <li><code>is_equal_to</code>: Ensures value present in a given field is exactly same as value provided by user in schema definition. </li> <li><code>is_less_than</code>: Ensures value present in a given field is less than value provided by user in schema definition.</li> </ol> <p>More rules will be added in subsequent releases for users to use directly in their schema. </p>"},{"location":"concepts/schema_validator/#rules-for-using-schema-validation-","title":"Rules for using schema validation:-","text":"<p>Now that we have covered a sample example on how to define schema and use it, here are some rules users have to keep in mind:- </p> <ol> <li>Schema validation is skipped if <code>schema_config</code> key is not present in <code>graph_config.yaml</code>. It is assumed that    user doesn't want schema validation to happen, hence we skip validation check in this case. </li> <li>If <code>schema_config</code> key is present in <code>graph_config.yaml</code>, it is expected that either <code>schema</code> or <code>fields</code> key is present inside <code>schema_config</code> and has been defined correctly. Absence of both or invalid definition of <code>schema</code> path or <code>fields</code> will raise exception. </li> <li><code>type</code> defined in either <code>custom_schemas.py</code> or inside <code>fields</code> have to be valid python types. Typo while defining type, for example <code>lisr</code> instead of <code>list</code> will raise invalid type error stopping the pipeline execution, and user has to re-define correctly. </li> </ol>"},{"location":"concepts/structured_output/","title":"Structured Output Generation","text":""},{"location":"concepts/structured_output/#introduction","title":"\ud83d\udccc Introduction","text":"<p>This module introduces a flexible and extensible framework to generate and validate structured outputs from LLMs. It is designed to work natively with Azure OpenAI, VLLM and Ollama, leveraging their structured output capabilities, while providing fallback support for any other LLM or custom model wrapper through JSON schema-based validation.</p>"},{"location":"concepts/structured_output/#why-this-is-helpful","title":"\ud83d\udd27 Why This is Helpful","text":"<ul> <li>Ensures consistent and reliable response formats from LLMs during data generation to reduce the post processing effort.</li> <li>Eliminates the need for brittle regex or manual parsing of raw LLM outputs.</li> <li>Allows easy plug-and-play validation via Pydantic classes or YAML-based schema configs.</li> </ul>"},{"location":"concepts/structured_output/#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>Structured Output Modes:</li> <li>Class-based schema: Define a Python class for validating output (e.g., using Pydantic).</li> <li>YAML-defined schema: Use simple YAML to define structure and field-level validation rules.</li> <li>Dynamic Type-Fetching:</li> <li>User simply needs to point their schema correctly to either of the above two choices. The type of schema desired and required processing for it is handled dynamically. </li> <li>Flexible Integration:</li> <li>Plug structured output validation into any LLM pipeline with minimal code changes.</li> <li>Easily extend by defining your own schemas or rules.</li> </ul>"},{"location":"concepts/structured_output/#architecture-overview","title":"\u2699\ufe0f Architecture Overview","text":"<p>Each node in a task graph can be defined with: - Model name (e.g., <code>gpt-4o</code>, <code>llama3-8b</code>) - Parameters (e.g., <code>temperature</code>) - Structured output config:   - Class-based (<code>type: class</code>)   - Schema-based (<code>type: schema</code>)</p> <pre><code>nodes:\n  node1:\n    node_type: llm\n    model:\n      name: gpt-4o\n      parameters:\n        temperature: 0.1\n    structured_output:\n      enabled: true\n      schema: \"grasp.core.models.structured_output.schemas_factory.CustomUserSchema\"\n\n  node2:\n    node_type: llm\n    model:\n      name: llama3-8b\n      parameters:\n        temperature: 0.2\n    structured_output:\n      enabled: true\n      schema:\n        fields:\n          answer:\n            type: str\n            description: \"The main answer\"\n          confidence:\n            type: float\n            description: \"Confidence score\"\n</code></pre>"},{"location":"concepts/structured_output/#how-to-define-a-schema","title":"\u270d\ufe0f How to Define a Schema","text":"<p>Structured output schema can be defined in two ways depending on your use case:</p>"},{"location":"concepts/structured_output/#option-1-using-python-class-pydantic","title":"\u2705 Option 1: Using Python Class (Pydantic)","text":"<p>This is the preferred approach if you want to write custom validation logic or reuse complex types.</p> <ol> <li>Define your schema class in <code>&lt;your_local_path&gt;</code>:</li> </ol> <pre><code>from pydantic import BaseModel, root_validator, model_validator, Field\nfrom typing import Any\n\n\nclass SimpleResponse(BaseModel):\n    \"\"\"Simple response with just text and status\"\"\"\n    message: str = Field(description=\"Response message\")\n    success: bool = Field(default=True, description=\"Operation success status\")\n\n   @model_validator(pre=True)\n   def check_non_empty_messages(cls, values):\n      if not values.get('message'):\n         raise ValueError('message cannot be empty')\n      return values\n</code></pre> <ol> <li>Then point to it in your YAML config:</li> </ol> <pre><code>structured_output:\n    enabled: true\n    schema: \"&lt;your_local_path&gt;.SimpleResponse\"\n</code></pre>"},{"location":"concepts/structured_output/#option-2-using-yaml-no-code-schema-definition","title":"\u2705 Option 2: Using YAML (No Code Schema Definition)","text":"<p>This is ideal for quick prototyping or use cases where no custom Python logic is needed. <pre><code> structured_output:\n   enabled: true\n   schema:\n     fields:\n       answer:\n         type: str\n         description: \"The main answer\"\n       confidence:\n         type: float\n         description: \"Confidence score\"\n</code></pre></p>"},{"location":"concepts/structured_output/#rules-for-using-schema-validation","title":"\ud83d\udccf Rules for Using Schema Validation","text":"<p>To ensure the structured output validation runs smoothly, follow these rules:</p>"},{"location":"concepts/structured_output/#general-rules","title":"\ud83d\udd39 General Rules","text":"<p>1. Structured Output Generation is triggered only when <code>structured_output</code> is defined:    - <code>schema</code> key must be present and must point to a valid pydantic class path or contain a dict defining desired output schema. Additional rules for class based schema definition or YAML based schema definition are described below. </p> <p>2. <code>enabled</code> is optional:    - If the <code>structured_output</code> key is present in the node\u2019s YAML config, <code>enabled</code> is set to true by default.    - Use this when you want to turn off <code>structured_output_generation</code> but want to preserve schema config make in <code>YAML</code>.</p>"},{"location":"concepts/structured_output/#for-class-based-schemas","title":"\ud83d\udd39 For Class-Based Schemas","text":"<ul> <li>Must inherit from <code>pydantic.BaseModel</code>.</li> <li>Can use <code>@model_validator</code> or <code>@validator</code> for custom rules.</li> <li>The class path provided must be fully qualified, e.g. <code>&lt;your_local_path&gt;&gt;.CustomUserSchema</code>.</li> </ul>"},{"location":"concepts/structured_output/#for-yaml-based-schemas","title":"\ud83d\udd39 For YAML-Based Schemas","text":"<ul> <li>Must be defined inside <code>fields</code> key inside <code>schema</code> key.</li> <li>Type for each field must be valid Python type.</li> </ul>"},{"location":"concepts/structured_output/#limitations","title":"\ud83d\udcdd Limitations","text":"<ul> <li>Native structured output generation is currently supported exclusively for <code>vllm</code>, <code>azure_openai</code>, <code>tgi</code> and <code>ollama</code> ModelTypes.</li> <li>For OpenAI ModelType, structured output functionality requires API versions <code>2024-08-01-preview</code> or later.</li> <li>For VLLM ModelType, structured output functionality requires version <code>0.8.4</code> or later.</li> <li>For all other ModelTypes, structured output generation is supported through fallback JSON schema validation.</li> </ul>"},{"location":"getting_started/Task_Executor/","title":"Task Executor","text":""},{"location":"getting_started/Task_Executor/#task-executor","title":"Task Executor","text":"<p>The Task Executor is responsible for executing tasks and managing their state.</p>"},{"location":"getting_started/Task_Executor/#basegraphstate-class","title":"<code>BaseGraphState</code> Class","text":"<p>This class defines the state of the graph. It is the base class for the state variables with some default variables. All the variables including the input data keys, variables defined within curly braces in the node_config, <code>output_vars</code>&amp; <code>output_key</code>defined in the node_config are available by default.</p> <p>The nodes communicate with each other using this class. It is mandatory to have <code>messages</code> field for using llm nodes.</p>"},{"location":"getting_started/Task_Executor/#init_graph-method","title":"<code>init_graph</code> Method","text":"<p>The init_graph method of the Task Executor is responsible for building and compiling the graph. Users need to use the <code>GraspStateGraph</code> class to get the <code>StateGraph</code> object.</p>"},{"location":"getting_started/Task_Executor/#init_datasetmethod","title":"<code>init_dataset</code>Method","text":"<p>The <code>init_dataset</code> method is responsible for initializing the dataset. It is called before the graph is built.</p>"},{"location":"getting_started/Task_Executor/#output_record_generator-method","title":"<code>output_record_generator</code> Method","text":"<p>The <code>output_record_generator</code> method is responsible for generating the output record. It is called after the graph is executed.</p>"},{"location":"getting_started/adding_edges/","title":"Adding Edges","text":""},{"location":"getting_started/adding_edges/#adding-edges-in-the-yaml-configuration","title":"Adding Edges in the YAML Configuration","text":""},{"location":"getting_started/adding_edges/#edge-configuration","title":"Edge Configuration","text":"<p>To add an edge to the graph configuration, include it in the edges section of the YAML file. Each edge is represented as a dictionary with specific properties that define its behavior. Here's an example of an edge configuration: <pre><code>edges:\n  - from: node1\n    to: node2\n  - from: node2\n    condition: tasks.example.task_executor.ShouldContinueCondition\n    path_map:\n      result1: node3\n      result2: node4\n</code></pre></p> <p>Properties  to configure an edge:</p>"},{"location":"getting_started/adding_edges/#special-nodes-start-and-end","title":"Special Nodes: START and END","text":"<p>GraSP graphs automatically include two special nodes:</p> <ul> <li>START: The entry point of the graph. Every graph must have at least one edge from START to another node.</li> <li>END: The exit point of the graph. When execution reaches the END node, the graph processing is complete.</li> </ul> <p>These special nodes are handled automatically by the framework and don't need to be defined in the <code>nodes</code> section. They are only referenced in edge definitions.</p> <p>Example: <pre><code>edges:\n  - from: START      # Entry point of the graph\n    to: first_node   # First node to execute\n  - from: last_node  # Last processing node\n    to: END          # Exit point of the graph\n</code></pre></p> <p>In conditional edges, you can direct flow to the END node to terminate processing: <pre><code>edges:\n  - from: some_node\n    condition: tasks.example.ShouldContinueCondition\n    path_map:\n      END: END                # Terminates processing when condition returns \"END\"\n      continue: next_node     # Continues to next_node when condition returns \"continue\"\n</code></pre></p>"},{"location":"getting_started/adding_edges/#from-required","title":"<code>from</code> (required)","text":"<p>The from property specifies the source node of the edge. It indicates the node from which the edge originates. The value should be a valid node name defined in the nodes section of the YAML configuration or the special node \"START\".</p> <p>Example: <pre><code>from: node1\n</code></pre></p>"},{"location":"getting_started/adding_edges/#to-optional","title":"<code>to</code> (optional)","text":"<p>The to property specifies the target node of the edge. It indicates the node to which the edge leads. The value should be a valid node name defined in the nodes section of the YAML configuration or the special node \"END\".</p> <p>Example: <pre><code>to: node2\n</code></pre></p>"},{"location":"getting_started/adding_edges/#condition-optional","title":"<code>condition</code> (optional)","text":"<p>The condition property allows to specify a condition functional class of type <code>EdgeCondition</code> that determines the path to take based on the result of the <code>apply</code> method.  The value should be a fully qualified path to the class. Example Code: <pre><code>class ShouldContinueCondition(EdgeCondition):\n    def apply(state: GraspState) -&gt; str:\n        # End after 4 iterations or the last feedback response contains \"NO MORE FEEDBACK\"\n        messages = state[\"messages\"]\n        if len(messages) &gt; 8 or (\n                len(messages) &gt; 1 and \"no more feedback\" in messages[-1].content.lower()\n        ):\n            return GRASP_END\n        return \"generate_answer\"\n</code></pre> <pre><code>condition: tasks.example.ShouldContinueCondition\n</code></pre> The <code>condition</code> class should accept the current state of the graph(<code>GraspState</code>) and return a value that matches one of the keys in the <code>path_map</code> (explained below).</p> <p>Alternatively, it also supports a direct method like <code>tasks.example.should_continue</code> for backward compatibility.</p>"},{"location":"getting_started/adding_edges/#path_map-optional","title":"<code>path_map</code> (optional)","text":"<p>The <code>path_map</code> property is used in conjunction with the <code>condition</code> property. It defines a mapping between the possible results of the condition function and the corresponding target nodes.</p> <p>Example: <pre><code>path_map:\n  result1: node3\n  result2: node4\n</code></pre> In this example, if the condition function returns result1, the edge will lead to node3. If the condition function returns result2, the edge will lead to node4.</p>"},{"location":"getting_started/adding_edges/#edge-validation","title":"Edge Validation","text":"<p>When configuring edges in the YAML file, it's important to ensure that the nodes referenced in the edges are valid and exist in the graph configuration. The graph builder will validate the edges and raise errors if any inconsistencies or invalid connections are found.</p> <p>Key considerations:</p> <ul> <li>The <code>from</code> node must be a valid node defined in the nodes section of the YAML configuration.</li> <li>If the <code>to</code> property is specified, it must refer to a valid node defined in the nodes section.</li> <li>If the <code>condition</code> and <code>path_map</code> properties are used, the <code>condition</code> function must exist, and the keys in the <code>path_map</code> must correspond to valid nodes.</li> </ul> <p>If any validation errors are encountered during the graph building process, a detailed error message will be raised, indicating the specific issues found in the edge configurations.</p>"},{"location":"getting_started/adding_edges/#examples","title":"Examples","text":"<p>Here are a few more examples of edge configurations to illustrate different scenarios:</p> <ol> <li>Simple edge: <pre><code>edges:\n  - from: node1\n    to: node2\n</code></pre></li> <li>Conditional edge with path mapping: <pre><code>edges:\n  - from: START\n    to: node1\n  - from: node1\n    to: node2\n  - from: node2\n    to: node3\n  - from: node3\n    condition: tasks.example.MyCondition\n    path_map:\n      success: node4\n      failure: node5\n</code></pre></li> </ol>"},{"location":"getting_started/create_new_pipeline/","title":"Create a Synthetic Datagen Pipeline","text":""},{"location":"getting_started/create_new_pipeline/#steps-to-create-a-synthetic-data-pipeline","title":"Steps to Create a Synthetic Data Pipeline","text":"<p>With the graph node and edge YAML configuration, it's easy to set up a flow. Example: glaive code assistant.</p> <p>Basic steps: - Create a sub-directory under <code>tasks</code> for your use case. - Create a <code>graph_config.yaml</code> for your pipeline (nodes, edges, models, etc). - Create a <code>task_executor.py</code> for any custom logic or processing. - Execute with <code>python main.py --task &lt;your_task&gt; ...</code> - Results are stored in <code>output.json</code> in your sub-directory.</p>"},{"location":"getting_started/create_new_pipeline/#resumable-execution","title":"Resumable Execution:","text":"<p>In the event of a failure, the process can gracefully shut down and later resume execution from the point of interruption. To activate resumable execution, set the flag <code>--resume True</code> when running your command. For instance: <code>python main.py --task &lt;your_task&gt; ... --resume True</code>. </p> <p>See the Graph Configuration Guide for detailed schema, examples, and best practices for defining graphs, tasks, and processors.</p>"},{"location":"getting_started/create_task_ui/","title":"Create a task with UI","text":""},{"location":"getting_started/create_task_ui/#run-the-ui-service","title":"Run the UI Service","text":"<p>The UI for this project is built using Streamlit and is located in the <code>apps</code> directory. To launch the GraSP UI locally, use the provided shell script:</p> <pre><code>./run_ui.sh\n</code></pre> <p>If you're running it for the first time, make sure the script is executable: <pre><code>chmod +x run_ui.sh\n</code></pre></p> <p>To run it on a custom port (e.g., 8502): <pre><code>./run_ui.sh 8502\n</code></pre> By default, the app will be available at: http://localhost:8501</p>"},{"location":"getting_started/create_task_ui/#steps-to-create-task","title":"Steps to create task","text":"<p>The Streamlit-based user interface provides a comprehensive set of tools to manage models and configure task flows in an interactive manner. Below are the key features:</p>"},{"location":"getting_started/create_task_ui/#1-model-management","title":"1. Model Management","text":"<p>Users can view all registered models along with their current status (active or inactive). The interface allows manual refreshing of model statuses to ensure accuracy. Additionally, users can register new models by providing essential details such as base URL, model name, type, and any custom configuration parameters.</p>"},{"location":"getting_started/create_task_ui/#2-review-existing-tasks","title":"2. Review Existing Tasks","text":"<p>Users can explore previously defined task flows through an interactive visual interface. This includes: - Viewing the task's directed graph structure - Inspecting individual node configurations - Understanding the data flow and logic for each task</p>"},{"location":"getting_started/create_task_ui/#3-create-a-new-task-flow-from-scratch","title":"3. Create a New Task Flow from Scratch","text":"<p>The UI guides users through the complete process of creating a new task flow: - Filling in <code>data_config</code> parameters - Constructing the task graph by defining nodes and edges - Defining the <code>output_config</code> section - Automatically generating the required <code>graph_config.yaml</code> and <code>task_executor.py</code> files - Reviewing and publishing the complete task setup</p>"},{"location":"getting_started/create_task_ui/#4-create-a-new-task-flow-based-on-existing-flows","title":"4. Create a New Task Flow Based on Existing Flows","text":"<p>Users can use existing task flows as templates, modify them as needed, and publish new customized task flows. This streamlines the task creation process by leveraging previously defined components.</p>"},{"location":"getting_started/graph_config_guide/","title":"GraSP Graph Configuration Guide","text":""},{"location":"getting_started/graph_config_guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Structure Overview</li> <li>Data Configuration</li> <li>Graph Configuration</li> <li>Nodes</li> <li>Edges<ul> <li>Simple Edges</li> <li>Conditional Edges</li> </ul> </li> <li>Output Configuration</li> <li>Full Example</li> <li>Schema_Validator</li> </ul>"},{"location":"getting_started/graph_config_guide/#structure-overview","title":"Structure Overview","text":"<p>A GraSP configuration file is a YAML document with these main sections:</p> <pre><code>data_config:\n  # Source data configuration\n\ngraph_config:\n  nodes:\n    # Node definitions\n  edges:\n    # Edge definitions\n\noutput_config:\n  # Output generation configuration\n\nschema_config:\n  # Output schema validation \n</code></pre> <p>Let's look at each section in detail. The sections below document all the available options and properties for each part of the configuration.</p>"},{"location":"getting_started/graph_config_guide/#data-configuration","title":"Data Configuration","text":"<p>The <code>data_config</code> section defines your input data sources, output destinations (sinks), and any transformations to apply.</p> <pre><code>data_config:\n  source:\n    # Example 1: HuggingFace dataset source\n    type: \"hf\"                               # HuggingFace dataset\n    repo_id: \"google-research-datasets/mbpp\" # HuggingFace repository ID\n    config_name: \"sanitized\"                 # Dataset configuration name\n    split: [\"train\", \"validation\", \"prompt\"] # Dataset splits to use\n\n    # OR\n\n    # Example 2: Local file source\n    type: \"disk\"                             # Local file source\n    file_path: \"/path/to/data.json\"          # Path to input file\n    file_format: \"json\"                      # Format (json, jsonl, csv, parquet)\n    encoding: \"utf-8\"                        # File encoding\n\n    # Optional transformations to apply to the input data\n    transformations:\n      - transform: grasp.processors.data_transform.RenameFieldsTransform  # Path to transformation class\n        params:                                                     # Parameters for the transformation\n          mapping:\n            task_id: id                     # Rename 'task_id' field to 'id'\n          overwrite: false                  # Don't overwrite existing fields\n\n  # Optional sink configuration for where to store output data\n  sink:\n    # Example 1: HuggingFace dataset sink\n    type: \"hf\"                               # HuggingFace dataset\n    repo_id: \"output-dataset/synthetic-mbpp\" # Where to upload the data\n    split: \"train\"                           # Split to write to\n    private: true                            # Create a private dataset\n\n    # OR\n\n    # Example 2: Local file sink\n    type: \"json\"                             # File format (json, jsonl, csv, parquet)\n    file_path: \"/path/to/output/file.json\"   # Path to save the file\n    encoding: \"utf-8\"                        # File encoding\n</code></pre>"},{"location":"getting_started/graph_config_guide/#data-source-options","title":"Data Source Options","text":"<p>The <code>source</code> subsection of <code>data_config</code> configures where the input data will come from.</p>"},{"location":"getting_started/graph_config_guide/#huggingface-source","title":"HuggingFace Source","text":"Parameter Type Description Default <code>type</code> string Source type: \"hf\" Required <code>repo_id</code> string HuggingFace dataset repository ID Required <code>config_name</code> string Dataset configuration name None <code>split</code> string or list Dataset split(s) to use \"train\" <code>token</code> string HuggingFace API token None <code>streaming</code> boolean Whether to stream the dataset false <code>shard</code> object Configuration for sharded processing None"},{"location":"getting_started/graph_config_guide/#local-file-source","title":"Local File Source","text":"Parameter Type Description Default <code>type</code> string Source type: \"disk\" Required <code>file_path</code> string Path to local file Required <code>file_format</code> string File format (json, jsonl, csv, parquet) Required <code>encoding</code> string Character encoding \"utf-8\""},{"location":"getting_started/graph_config_guide/#transformations","title":"Transformations","text":"<p>Transformations allow you to modify the input data before processing.</p> Parameter Type Description <code>transform</code> string Fully qualified path to a transformation class <code>params</code> object Parameters for the transformation"},{"location":"getting_started/graph_config_guide/#some-of-the-available-transformations-are","title":"Some of the available transformations are:","text":""},{"location":"getting_started/graph_config_guide/#renamefieldstransform","title":"RenameFieldsTransform","text":"<p>It renames the fields in the dataset, so the prompt variables used are meaningful and reusable. The Below example shows how the <code>page</code> is renamed to <code>id</code>, <code>llm_extract</code> is renamed to <code>text</code> and <code>type</code> is renamed to <code>text_format</code>. <pre><code>      - transform: grasp.processors.data_transform.RenameFieldsTransform\n        params:\n          mapping:\n            page: id\n            llm_extract: text\n            type: text_format\n</code></pre></p>"},{"location":"getting_started/graph_config_guide/#combinerecords","title":"CombineRecords","text":"<p>When you want to combine records to form a new dataset, you can use this transformation. Below example shows how we can skip 10 records from beginning and from end, and combine 2 records by shifting 1. For example record <code>11</code> and <code>12</code> will be combined to form <code>page</code>=<code>11-12</code>, in this example, <code>pdf_reader</code> and <code>llm_extract</code> columns are combined with two new lines.  And <code>type</code>, <code>model</code>, <code>metadata</code> is just picking data from first record. <code>$1</code> denotes first record, <code>$2</code> denotes second record and so on.  Once <code>11</code> and <code>12</code> is combined to form <code>11-12</code>, it shift by 1 and combines <code>12</code> with <code>13</code> to form <code>12-13</code>. <pre><code>      - transform: grasp.processors.data_transform.CombineRecords\n        params:\n          skip:\n            from_beginning: 10\n            from_end: 10\n          combine: 2\n          shift: 1\n          join_column:\n            page: \"$1-$2\"\n            pdf_reader: \"$1\\n\\n$2\"\n            llm_extract: \"$1\\n\\n$2\"\n            type: \"$1\"\n            model: \"$1\"\n            metadata: \"$1\"\n</code></pre></p>"},{"location":"getting_started/graph_config_guide/#skiprecords","title":"SkipRecords","text":"<p>When we want to skip records for a dataset, we can use this transform. Below example shows how to skip first 10 and last 10 records using count. <pre><code>      - transform: grasp.processors.data_transform.SkipRecords\n        params:\n          skip_type: \"count\"\n          count:\n            from_start: 10\n            from_end: 10\n</code></pre> Below example shows how to skip first 10 and last 10 records using range. <pre><code>      - transform: grasp.processors.data_transform.SkipRecords\n        params:\n          skip_type: \"range\"\n          range: \"[:10],[-10:]\"\n</code></pre></p>"},{"location":"getting_started/graph_config_guide/#data-sink-options","title":"Data Sink Options","text":"<p>The <code>sink</code> subsection of <code>data_config</code> configures where the output data will be stored.</p>"},{"location":"getting_started/graph_config_guide/#huggingface-sink","title":"HuggingFace Sink","text":"Parameter Type Description Default <code>type</code> string Sink type: \"hf\" Required <code>repo_id</code> string HuggingFace dataset repository ID Required <code>config_name</code> string Dataset configuration name None <code>split</code> string Dataset split to write to \"train\" <code>token</code> string HuggingFace API token None <code>private</code> boolean Whether to create a private dataset true"},{"location":"getting_started/graph_config_guide/#file-sink","title":"File Sink","text":"Parameter Type Description Default <code>type</code> string File format: \"json\", \"jsonl\", \"csv\", \"parquet\" Required <code>file_path</code> string Path to output file Required <code>encoding</code> string Character encoding \"utf-8\""},{"location":"getting_started/graph_config_guide/#data-less-configuration","title":"Data Less Configuration","text":"<p>GraSP supports generating data without requiring an input data source. This is useful for knowledge distillation from models, creating purely synthetic datasets, or any scenario where you want to generate content from scratch.</p> <p>To generate data without a source:</p> <ol> <li>Simply omit the <code>source</code> configuration in the <code>data_config</code> section</li> <li>Keep the <code>sink</code> configuration to specify where to store the generated data</li> </ol> <pre><code>data_config:\n  # No source configuration\n\n  # Only sink configuration\n  sink:\n    type: \"json\"\n    file_path: \"output/synthetic_data.jsonl\"\n</code></pre>"},{"location":"getting_started/graph_config_guide/#graph-configuration","title":"Graph Configuration","text":"<p>The <code>graph_config</code> section defines the nodes and edges of your computational graph.</p> <pre><code>graph_config:\n  nodes:\n    # Node definitions\n  edges:\n    # Edge definitions\n</code></pre> <p>This section is where you define the processing steps (nodes) and the flow between them (edges) that make up your data generation pipeline.</p>"},{"location":"getting_started/graph_config_guide/#graph-properties","title":"Graph Properties","text":"<p>This defines the graph level properties, it can be a common properties but controlled from the task. <pre><code>  graph_properties:\n    chat_conversation: singleturn   #singleturn or multiturn\n    chat_history_window_size: 5\n</code></pre></p>"},{"location":"getting_started/graph_config_guide/#nodes","title":"Nodes","text":"<p>Nodes represent the processing steps in your pipeline. GraSP supports multiple types of nodes, such as LLM, multi_llm, weighted_sampler, lambda, agent, subgraph, and more.</p> <p>All node types support these common parameters:</p> Parameter Type Description Default <code>node_type</code> string Type of node (\"llm\", \"multi_llm\", \"weighted_sampler\", \"lambda\", etc.) Required <code>node_state</code> string Node state (\"active\" or \"idle\") to enable/disable the node \"active\" <p>For detailed documentation and configuration options for each node type, see nodes/.</p>"},{"location":"getting_started/graph_config_guide/#edges","title":"Edges","text":"<p>Edges define the flow of execution between nodes.</p>"},{"location":"getting_started/graph_config_guide/#special-nodes-start-and-end","title":"Special Nodes: START and END","text":"<p>GraSP graphs automatically include two special nodes:</p> <ul> <li>START: The entry point of the graph. Every graph must have at least one edge from START to another node.</li> <li>END: The exit point of the graph. When execution reaches the END node, the graph processing is complete.</li> </ul> <p>These special nodes are handled automatically by the framework and don't need to be defined in the <code>nodes</code> section. They are only referenced in edge definitions.</p>"},{"location":"getting_started/graph_config_guide/#simple-edges","title":"Simple Edges","text":"<p>Simple edges define a direct path from one node to another.</p> <pre><code>edges:\n  - from: START              # Source START node (entry point)\n    to: persona_sampler      # Target node\n  - from: persona_sampler    # Source node\n    to: paraphrase_question  # Target node\n  - from: final_node         # Last processing node\n    to: END                  # Exit point of the graph\n</code></pre>"},{"location":"getting_started/graph_config_guide/#conditional-edges","title":"Conditional Edges","text":"<p>Conditional edges define different paths based on a condition. Conditions can direct flow to the END node to terminate processing.</p> <pre><code>- from: critique_answer\n  condition: tasks.mbpp.code_generation_with_graph_builder.task_executor.ShouldContinueCondition\n  path_map:\n    END: END                          # Path to END when condition returns \"END\" (terminates processing)\n    generate_answer: generate_answer  # Path to generate_answer when condition returns \"generate_answer\"\n</code></pre> <p>In condition functions, you can return <code>constants.GRASP_END</code> to direct flow to the END node:</p> <pre><code>class ShouldContinueCondition(EdgeCondition):\n    def apply(self, state: GraspState) -&gt; str:\n        # End after 4 iterations or the last feedback response contains \"NO MORE FEEDBACK\"\n        messages = state[\"messages\"]\n        if len(messages) &gt; 8 or (\n                len(messages) &gt; 1 and \"no more feedback\" in messages[-1].content.lower()\n        ):\n            return constants.GRASP_END  # This will direct flow to the END node\n        return \"generate_answer\"\n</code></pre> <p>Edge Parameters:</p> Parameter Type Description <code>from</code> string Source node name (can be a regular node or START) <code>to</code> string Target node name (can be a regular node or END) <code>condition</code> string Fully qualified path to a condition class or function (for conditional edges) <code>path_map</code> object Map of condition results to target node names (for conditional edges)"},{"location":"getting_started/graph_config_guide/#output-configuration","title":"Output Configuration","text":"<p>The <code>output_config</code> section defines how to generate the final output records. This component translates the final state of the graph into the desired output format for each processed record.</p>"},{"location":"getting_started/graph_config_guide/#overview","title":"Overview","text":"<p>There are two approaches to generating output records:</p> <ol> <li>YAML-driven with the <code>output_map</code> configuration (recommended)</li> <li>Custom Python implementation by overriding the <code>generate()</code> method</li> </ol>"},{"location":"getting_started/graph_config_guide/#yaml-driven-output-configuration","title":"YAML-Driven Output Configuration","text":"<p>This approach uses declarative configuration to map state variables to output fields. The <code>output_map</code> section defines how to construct your final output records by specifying what goes into each field:</p> <pre><code>output_config:\n  # Path to a class that inherits from BaseOutputGenerator\n  generator: tasks.mbpp.code_generation_with_graph_builder.task_executor.CodeGenOutputGenerator\n\n  # Map of output fields and how to populate them\n  output_map:\n    id:                       # Output field name\n      from: \"id\"              # Get value from this state variable\n    conversation:\n      from: \"messages\"        # Get value from this state variable\n      transform: \"build_conversation\"  # Apply this method from the generator class to transform the value\n    taxonomy:\n      value:                  # Use this static value (not from state)\n        - category: \"Coding\"\n          subcategory: \"\"\n    annotation_type:\n      value: [\"mistral-large\"]\n    language:\n      value: \"en\"\n    tags:\n      value: [\"mbpp\", \"self-critique\"]\n</code></pre>"},{"location":"getting_started/graph_config_guide/#how-output_map-works","title":"How <code>output_map</code> works","text":"<p>The <code>output_map</code> is a dictionary where: 1. Each key becomes a field name in your output record 2. Each value is a configuration object that defines how to populate that field</p> <p>For each field, you have two main ways to populate it:</p> <ol> <li> <p>Dynamic values from state (using <code>from</code>):    <pre><code>id:\n  from: \"id\"  # Takes the value from state[\"id\"]\n</code></pre>    This retrieves the value with the key \"id\" from the graph state and puts it in the output record's \"id\" field.</p> </li> <li> <p>Static values (using <code>value</code>):    <pre><code>language:\n  value: \"en\"  # Hardcoded value \"en\"\n</code></pre>    This puts the literal value \"en\" in the output record's \"language\" field.</p> </li> <li> <p>Transformed values (using <code>from</code> + <code>transform</code>):    <pre><code>conversation:\n  from: \"messages\"  # Takes the value from state[\"messages\"]\n  transform: \"build_conversation\"  # Passes it through a transformation method\n</code></pre>    This takes the value from state[\"messages\"], passes it through the <code>build_conversation</code> method defined in your generator class, and puts the result in the output record's \"conversation\" field.</p> </li> </ol>"},{"location":"getting_started/graph_config_guide/#example-output-record","title":"Example output record","text":"<p>With the configuration above, your final output record would look like:</p> <pre><code>{\n  \"id\": \"mbpp-125\",  // Value from state[\"id\"]\n  \"conversation\": [  // Result of build_conversation(state[\"messages\"])\n    {\"role\": \"user\", \"content\": \"Write a function to check if a number is prime\"},\n    {\"role\": \"assistant\", \"content\": \"Here's a function...\"}\n  ],\n  \"taxonomy\": [  // Static value from configuration\n    {\n      \"category\": \"Coding\",\n      \"subcategory\": \"\"\n    }\n  ],\n  \"annotation_type\": [\"mistral-large\"],  // Static value\n  \"language\": \"en\",  // Static value\n  \"tags\": [\"mbpp\", \"self-critique\"]  // Static value\n}\n</code></pre> <p>Output Configuration Parameters:</p> Parameter Type Description <code>generator</code> string Fully qualified path to a class that inherits from <code>BaseOutputGenerator</code> <code>output_map</code> object Map of output field names to mappings <code>output_map.&lt;field&gt;.from</code> string State variable to get value from (mutually exclusive with <code>value</code>) <code>output_map.&lt;field&gt;.value</code> any Static value for the field (mutually exclusive with <code>from</code>) <code>output_map.&lt;field&gt;.transform</code> string Method name in the generator class to transform the value <p>Metadata in Output Map</p> <p>Metadata can be any supported data for the record, sometimes we want to put datasource as metadata. However, datasource is already mentioned in the current YAML file. output_map value supports \\$ variables which points to a node in the YAML. \\$ variables are only supported under <code>value</code> key.</p> <p>Below example shows how a dictionary value can have \\$ variables as dictionary values, list values and direct string value.  It can read the path with dot format, also supports list with subscript operator.</p> <pre><code>output_config:\n  output_map:\n    id:\n      from: \"id\"\n    content:\n      from: \"text\"\n    metadata:\n      value:\n        source:\n          - type: [$data_config.source.type, $data_config.source.config_name]\n            location: $data_config.source.repo_id\n          - type: $graph_config.nodes.extract_question.node_type\n            location: $graph_config.nodes.extract_question.model.name\n        start_node: $graph_config.edges[0].from\n        author: john doe\n</code></pre>"},{"location":"getting_started/graph_config_guide/#custom-transformations","title":"Custom Transformations","text":"<p>When using <code>transform</code> in the <code>output_map</code>, you must implement the corresponding method in your generator class:</p> <pre><code>class CodeGenOutputGenerator(BaseOutputGenerator):\n    \"\"\"\n    Example output generator with custom transformations\n    \"\"\"\n    def build_conversation(self, data: Any, state: dict[str, Any]) -&gt; Any:\n        \"\"\"\n        Transform messages into a conversation format\n\n        Args:\n            data: The value from the state (from the 'from' field)\n            state: The entire graph state\n\n        Returns:\n            The transformed value\n        \"\"\"\n        chat_format_messages = utils.convert_messages_from_langchain_to_chat_format(data)\n\n        # Example transformation logic:\n        if chat_format_messages and \"no more feedback\" in chat_format_messages[-1][\"content\"].lower():\n            # Remove the last message with \"no more feedback\"\n            chat_format_messages = chat_format_messages[:-1]\n\n        # Add additional messages or modify existing ones\n        if \"rephrased_text\" in state and state[\"rephrased_text\"]:\n            # output keys can be directly accessed from state\n            question = state[\"rephrased_text\"].replace(\"PARAPHRASED QUESTION: \", \"\")\n            chat_format_messages.insert(0, {\"role\": \"user\", \"content\": question})\n\n        return chat_format_messages\n</code></pre>"},{"location":"getting_started/graph_config_guide/#fully-custom-output-generation","title":"Fully Custom Output Generation","text":"<p>For more complex output generation logic, you can override the <code>generate()</code> method:</p> <pre><code>class CustomOutputGenerator(BaseOutputGenerator):\n    def generate(self, state: GraspState) -&gt; dict[str, Any]:\n        \"\"\"\n        Create a custom output record from the graph state\n\n        Args:\n            state: The final graph state\n\n        Returns:\n            The output record as a dictionary\n        \"\"\"\n        # Custom logic to build the output record\n        if \"messages\" not in state:\n            return None  # Skip records that don't have messages\n\n        # Build your output record with custom logic\n        record = {\n            \"id\": state.get(\"id\", \"\"),\n            \"conversation\": self._process_conversation(state[\"messages\"]),\n            \"metadata\": self._build_metadata(state),\n            # Other fields...\n        }\n\n        return record\n\n    def _process_conversation(self, messages):\n        # Helper method for processing messages\n        # ...\n\n    def _build_metadata(self, state):\n        # Helper method for building metadata\n        # ...\n</code></pre> <p>The output generator is the final step in the pipeline and determines what data gets saved as the result of your synthetic data generation process.</p>"},{"location":"getting_started/graph_config_guide/#full-example","title":"Full Example","text":"<p>Here's a complete example based on the code generation task:</p> <pre><code>data_config:\n  source:\n    type: \"hf\"\n    repo_id: \"google-research-datasets/mbpp\"\n    config_name: \"sanitized\"\n    split: [\"train\", \"validation\", \"prompt\"]\n\n    transformations:\n      - transform: grasp.processors.data_transform.RenameFieldsTransform\n        params:\n          mapping:\n            task_id: id\n          overwrite: false\n\ngraph_config:\n  nodes:\n    persona_sampler:\n      node_type: weighted_sampler\n      attributes:\n        num_turns:\n          values: [2, 3, 4, 5]\n        tone1:\n          values: [professional, casual, friendly, inquisitive, formal]\n        persona1:\n          values: [high school teacher, college professor, software engineer]\n\n    paraphrase_question:\n      node_type: llm        \n      output_keys: rephrased_text\n      prompt:\n        - system: |\n            Assume you are {persona1} persona.\n            You are an assistant tasked with paraphrasing a user question.\n        - user: |\n            QUESTION: {prompt}. Write the program in python.      \n      model:      \n        name: mistralai      \n        parameters:          \n          temperature: 1.0\n\n    generate_answer:  \n      node_type: llm        \n      prompt:\n        - system: |\n            You are an assistant tasked with solving python coding problems.\n        - user: |\n            {prompt}      \n      model:      \n        name: gpt-4o            # Must match a model defined in config/models.yaml\n        parameters:             # Override default parameters from models.yaml\n          temperature: 0.1\n\n    critique_answer:  \n      pre_process: tasks.mbpp.code_generation_with_graph_builder.task_executor.CritiqueAnsNodePreProcessor\n      node_type: llm \n      output_role: user \n      prompt:        \n        - system: |              \n            You are a teacher grading a solution to a python coding problem.\n\n            QUESTION: {prompt}            \n            TEST CASES: {test_list}\n      model:      \n        name: gpt-4o\n        parameters:          \n          temperature: 1.0\n\n  edges:\n    - from: START\n      to: persona_sampler\n    - from: persona_sampler\n      to: paraphrase_question\n    - from: paraphrase_question\n      to: generate_answer\n    - from: generate_answer\n      to: critique_answer\n    - from: critique_answer\n      condition: tasks.mbpp.code_generation_with_graph_builder.task_executor.ShouldContinueCondition\n      path_map:\n        END: END\n        generate_answer: generate_answer\n\noutput_config:\n  generator: tasks.mbpp.code_generation_with_graph_builder.task_executor.CodeGenOutputGenerator\n\n  output_map:\n    id:\n      from: \"id\"\n    conversation:\n      from: \"messages\"\n      transform: \"build_conversation\"\n    taxonomy:\n      value:\n        - category: \"Coding\"\n          subcategory: \"\"\n    annotation_type:\n      value: [\"mistral-large\"]\n    language:\n      value: \"en\"\n    tags:\n      value: [\"mbpp\", \"reannotate\", \"self-critique\"]\n</code></pre>"},{"location":"getting_started/graph_config_guide/#schema-validator","title":"Schema Validator","text":""},{"location":"getting_started/graph_config_guide/#introduction","title":"Introduction","text":"<p>Schema validator enables users to ensure correctness of generated data before uploading to HF or File System.</p> <p>Key features supported for schema validation are as follows: </p> <ol> <li>YAML based schema check: Users can define their schema using YAML config files in the following ways:-</li> <li>Define a custom schema class inside <code>custom_schemas.py</code> and add it's path in <code>schema</code> key inside <code>schema_config</code>.</li> <li> <p>Add expected schema config in a list of dict format inside <code>fields</code> key inside <code>schema_config</code>.</p> </li> <li> <p>Rule based validation support: Aside from adding validator rules inside custom class, users can choose from validation methods supported(details in additional validation rules section) and add it as a key for a particular field's dict.</p> </li> </ol>"},{"location":"getting_started/graph_config_guide/#usage-illustration","title":"Usage Illustration","text":"<p>Let's assume we have the following record generated which we want to validate: </p> <p><pre><code>{\n        \"id\": 130426,\n        \"conversation\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"I am trying to get the CPU cycles at a specific point in my code.\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"The `rdtsc` function you're using gives you the number of cycles since the CPU was last reset, which is not what you want in this case.\"\n            }\n        ],\n        \"taxonomy\": [\n            {\n                \"category\": \"Coding\",\n                \"subcategory\": \"\"\n            }\n        ],\n        \"annotation_type\": [\n            \"mistral-large\"\n        ],\n        \"language\": [\n            \"en\"\n        ],\n        \"tags\": [\n            \"glaiveai/glaive-code-assistant-v2\",\n            \"reannotate\",\n            \"self-critique\"\n        ]\n}\n</code></pre> For the above record, user can have the following class defined inside <code>custom_schemas.py</code> defining the  expected keys and values along with additional validation rules if any. </p> <pre><code>class CustomUserSchema(BaseModel):\n    '''\n    This demonstrates an example of a customizable user schema that can be modified or redefined by the end user.\n    Below is a sample schema with associated validator methods.\n    '''\n    id: int\n    conversation: list[dict[str,Any]]\n    taxonomy: list[dict[str, Any]]\n    annotation_type: list[str]\n    language: list[str]\n    tags: list[str]\n\n    @root_validator(pre=True)\n    def check_non_empty_lists(cls, values):\n        if not values.get('id'):\n            raise ValueError('id cannot be empty')\n        return values\n</code></pre>"},{"location":"getting_started/graph_config_guide/#sample-yaml-configuration-to-use-custom-schema-defined-in-custom_schemaspy","title":"Sample YAML configuration to use custom schema defined in <code>custom_schemas.py</code>","text":"<pre><code>schema_config:\n  schema: grasp.validators.custom_schemas.CustomUserSchema\n</code></pre>"},{"location":"getting_started/graph_config_guide/#sample-yaml-configuration-to-define-schema-in-yaml","title":"Sample YAML configuration to define schema in YAML:","text":"<p><pre><code>schema_config:\n  fields:\n    - name: id\n      type: int\n      is_greater_than: 99999\n    - name: conversation\n      type: list[dict[str, any]]\n    - name: taxonomy\n      type: list[dict[str, any]]\n    - name: annotation_type\n      type: list[str]\n    - name: language\n      type: list[str]\n    - name: tags\n      type: list[str]\n</code></pre> Note that <code>fields</code> is expected to be a list of dicts with <code>name</code> and <code>type</code> present in each dict with additional option of providing validation key. In the above example <code>is_greater_than</code> is a validation key shown for demonstration purpose  to ensure <code>id</code> key in each record has a value with 6 digits or more. </p>"},{"location":"getting_started/graph_config_guide/#post-generation-tasks","title":"Post Generation Tasks","text":"<p>Post generation tasks are tasks that are executed after the graph has been executed. These tasks can be used to perform additional processing on the generated data, such as OASST Mapper and Data Quality tagging. </p>"},{"location":"getting_started/graph_config_guide/#data-mapper-or-oasst_mapper","title":"<code>Data mapper</code> or <code>oasst_mapper</code>","text":"<p>OASST Mapper enables users to transform data coming from output record generator into SFT/DPO format depending upon user's choice in the OASST2 fomat.</p> <p>By default, the Data Mapper is disabled. To enable it, add the following runtime argument:</p> <pre><code>--oasst True\n</code></pre> <p>You can refer to Data Mapper for more details on how to configure the OASST Mapper.</p>"},{"location":"getting_started/graph_config_guide/#data-quality-tagging","title":"<code>Data Quality</code> tagging","text":"<p>Data Quality tagging is a feature that allows users to tag the generated data with quality metrics which can be useful for evaluation of the generated data and act as a filtering mechanism during training.</p> <p>By default, the Data Quality tagging is disabled. To enable it, add the following runtime argument:</p> <p><pre><code>--quality True\n</code></pre> You can refer to Data Quality Tagging for more details on how to configure the Data Quality tagging.</p>"},{"location":"getting_started/model_configuration/","title":"Model Configuration","text":"<p>GraSP requires model configuration as the first step. It supports various clients like HTTP, MistralAzure, AsyncOpenAI, AsyncAzureOpenAI, Ollama to connect to inference servers (Text Generation Inference (TGI), vLLM server, Azure Cloud Service, Ollama, Triton etc.).</p> <p>The <code>config</code> folder contains the main configuration file: <code>models.yaml</code>. You can add your model alias as a key and define its properties as shown below.</p> <p>Note: For Triton, the pre-processing and post-processing configuration (<code>payload_json</code> &amp; <code>response_key</code>) can be defined in the <code>payload_cfg.json</code> file. <code>payload_key</code> in the <code>payload_cfg.json</code> file should be added to the <code>models.yaml</code> file for the corresponding Triton model. If the payload key is not defined in <code>models.yaml</code>, the default payload format will be used.</p>"},{"location":"getting_started/model_configuration/#environment-variables-for-credentials-and-chat-templates","title":"Environment Variables for Credentials and Chat Templates","text":"<p>All sensitive connection information such as model URL and tokens must be set via environment variables and not stored in the config file.</p> <p>For each model defined in your <code>models.yaml</code>, set environment variables as follows: - <code>GRASP_&lt;MODEL_NAME&gt;_URL</code> (for the model endpoint) - <code>GRASP_&lt;MODEL_NAME&gt;_TOKEN</code> (for API keys or tokens) - If <code>modify_tokenizer: true</code> is set for a model, provide a chat template string via:   - <code>GRASP_&lt;MODEL_NAME&gt;_CHAT_TEMPLATE</code></p> <p>Naming Convention: <code>&lt;MODEL_NAME&gt;</code> is the model\u2019s key from your <code>models.yaml</code>, with all spaces replaced by underscores, and all letters uppercased (e.g., <code>mixtral 8x7b</code> \u2192 <code>MIXTRAL_8X7B</code>).</p> <p>Example: For <code>mixtral_8x7b</code> and <code>gpt4</code>, set: - <code>GRASP_MIXTRAL_8X7B_URL</code>, <code>GRASP_MIXTRAL_8X7B_TOKEN</code> - <code>GRASP_GPT4_URL</code>, <code>GRASP_GPT4_TOKEN</code> - If <code>mixtral_8x7b</code> has <code>modify_tokenizer: true</code>, set:   - <code>GRASP_MIXTRAL_8X7B_CHAT_TEMPLATE</code> to your custom Jinja2 chat template string</p> <p>You should use a <code>.env</code> file at the project root or set these in your shell environment.</p> <p>Note: If you want to define a list of URLs for any model, you can use pipe (<code>|</code>) as a separator. For example, if you have a model called <code>mixtral_8x7b</code> with URLs <code>https://myserver/models/mixtral-8x7b</code> and <code>https://myserver/models/mixtral-8x7b-2</code>, you can set the following environment variables as shown in examples below. </p>"},{"location":"getting_started/model_configuration/#example-env","title":"Example <code>.env</code>:","text":"<pre><code>GRASP_MIXTRAL_8X7B_URL=https://myserver/models/mixtral-8x7b|https://myserver/models/mixtral-8x7b-2\nGRASP_MIXTRAL_8X7B_TOKEN=sk-abc123\nGRASP_MIXTRAL_8X7B_CHAT_TEMPLATE={% for m in messages %} ... {% endfor %}\n</code></pre>"},{"location":"getting_started/model_configuration/#configuration-properties","title":"Configuration Properties","text":"Key Description <code>model_type</code> Type of backend server (<code>tgi</code>, <code>vllm</code>, <code>azure_openai</code>, <code>azure</code>, <code>mistralai</code>, <code>ollama</code>, <code>triton</code>) <code>model_name</code> Model name for your deployments (for Azure/Azure OpenAI) <code>api_version</code> API version for Azure or Azure OpenAI <code>hf_chat_template_model_id</code> Hugging Face model ID <code>completions_api</code> (Optional) Boolean: use completions API instead of chat completions API (default: false) <code>modify_tokenizer</code> (Optional) Boolean: apply custom chat template and modify the base model tokenizer (default: false) <code>special_tokens</code> (Optional) List of special stop tokens used in generation <code>post_process</code> (Optional) Post processor after model inference (e.g. <code>models.model_postprocessor.RemoveThinkData</code>) <code>parameters</code> (Optional) Generation parameters (see below) <code>ssl_verify</code> (Optional) Verify SSL certificate (default: true) <code>ssl_cert</code> (Optional) Path to SSL certificate file &gt; Note: &gt; - Do not include <code>url</code>, <code>auth_token</code>, or <code>api_key</code> in your YAML config. These are sourced from environment variables as described above. &gt; - If you want to set ssl_verify to false globally, you can set <code>ssl_verify:false</code> under <code>model_config</code> section in config/configuration.yaml #### Customizable Model Parameters <ul> <li><code>temperature</code>: Sampling randomness (0.0\u20132.0; lower is more deterministic)</li> <li><code>top_p</code>: Nucleus sampling (0.0\u20131.0)</li> <li><code>max_tokens</code> / <code>max_new_tokens</code>: Maximum number of tokens to generate</li> <li><code>stop</code>: List of stop strings to end generation</li> <li><code>repetition_penalty</code>: Penalizes repeated tokens (1.0 = no penalty)</li> <li><code>presence_penalty</code>: (OpenAI only) Encourages novel tokens</li> <li><code>frequency_penalty</code>: (OpenAI only) Penalizes frequently occurring tokens</li> </ul> <p>The model alias set as a key in the configuration is referenced in your graph YAML files (for node types such as <code>llm</code> or <code>multi_llm</code>). You can override these model parameters in the graph YAML for specific scenarios.</p>"},{"location":"getting_started/model_configuration/#example-configuration-modelsyaml","title":"Example Configuration (<code>models.yaml</code>)","text":"<pre><code>mixtral_8x7b:\n  model_type: vllm\n  hf_chat_template_model_id: meta-llama/Llama-2-7b-chat-hf\n  modify_tokenizer: true\n  parameters:\n    temperature: 0.7\n    top_p: 0.9\n    max_new_tokens: 2048\n\ngpt4:\n  model_type: azure\n  model_name: gpt-4-32k\n  api_version: 2024-05-01-preview\n  parameters:\n    max_tokens: 500\n    temperature: 1.0\n\nqwen_2.5_32b_vl:\n  model_type: vllm\n  completions_api: true\n  hf_chat_template_model_id: Qwen/Qwen2.5-VL-32B-Instruct\n  parameters:\n    temperature: 0.15\n    max_tokens: 10000\n    stop: [\"&lt;|endoftext|&gt;\", \"&lt;|im_end|&gt;\", \"&lt;|eod_id|&gt;\"]\n\nqwen3_1.7b:\n  hf_chat_template_model_id: Qwen/Qwen3-1.7B\n  post_process: grasp.core.models.model_postprocessor.RemoveThinkData\n  model_type: ollama\n  parameters:\n    max_tokens: 2048\n    temperature: 0.8\n\nqwen3-32b-triton:\n  hf_chat_template_model_id: Qwen/Qwen3-32B\n  post_process: grasp.core.models.model_postprocessor.RemoveThinkData\n  model_type: triton\n  payload_key: default \n  # Uses default payload format defined in config/payload_cfg.json.\n  # Add/Update the payload_cfg.json if you need to use a different payload format with new key.\n  parameters:\n    temperature: 0.7\n</code></pre> <p>Important: If you set modify_tokenizer: true for a model, you must provide the corresponding chat template in your environment as GRASP__CHAT_TEMPLATE. Otherwise, exception will be raised during the model initialization."}]}