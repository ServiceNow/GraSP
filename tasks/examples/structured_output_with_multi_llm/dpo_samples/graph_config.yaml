data_config:
  source:
    type: "disk"
    file_path: "tasks/examples/structured_output_with_multi_llm/dpo_samples/dpo_conversation.json"
    transformations:
      - transform: tasks.examples.structured_output_with_multi_llm.dpo_samples.data_transform.ExtractUserPromptTransform
        params: {}
      - transform: tasks.examples.structured_output_with_multi_llm.dpo_samples.data_transform.ExtractResponseScaleTransform
        params: {}
      - transform: tasks.examples.structured_output_with_multi_llm.dpo_samples.data_transform.InitializeStateVariablesTransform
        params: {}


graph_config:
  nodes:
    generate_samples:
      node_type: multi_llm
      pre_process: tasks.examples.structured_output_with_multi_llm.dpo_samples.task_executor.GenerateSamplesPreProcessor
      output_keys: model_responses
      prompt:
        - user: "{user_prompt}"
      models:
        gpt4:
          name: gpt4
          parameters:
            temperature: 0.7
            max_tokens: 2000
          structured_output:
            schema:
              fields:
                message:
                  type: str
                  description: "Response message"
                success:
                  type: bool
                  description: "Operation success status"
        gpt-4o:
          name: gpt-4o
          parameters:
            temperature: 0.7
            max_tokens: 2000
          structured_output:
            schema: "grasp.core.models.structured_output.schemas_factory.SimpleResponse"
        gpt-4o-mini:
          name: gpt-4o-mini
          parameters:
            temperature: 0.7
            max_tokens: 2000
          structured_output:
            enabled: true
            schema:
              fields:
                message:
                  type: str
                  description: "Response message"
                success:
                  type: bool
                  description: "Operation success status"

    rate_samples:
      node_type: llm
      pre_process: tasks.examples.structured_output_with_multi_llm.dpo_samples.task_executor.RateSamplesPreProcessor
      post_process: tasks.examples.structured_output_with_multi_llm.dpo_samples.task_executor.RateSamplesPostProcessor
      prompt:
        - system: You are a helpful assistant
        - user: |
            [Instruction]
            Please act as an impartial judge and evaluate the quality of the responses provided by multiple AI assistant(s) to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Provide a short explanation for your evaluation. Be as objective as possible. Rate the response on a scale of 1 to 10 by strictly following this JSON format: 
            [{{"assistant": <assistant>, "rating": <rating>, "explanation": "<explanation>"}}]
                
            [Question]
            {user_prompt}
            
            {assistant_responses}
      model:
        name: gpt4
        parameters:
          max_tokens: 1000
          temperature: 0.1

  edges:
    - from: START
      to: generate_samples
    - from: generate_samples
      to: rate_samples
    - from: rate_samples
      condition: tasks.examples.structured_output_with_multi_llm.dpo_samples.task_executor.ShouldContinueCondition
      path_map:
        END: END
        generate_samples: generate_samples

output_config:
  generator: tasks.examples.structured_output_with_multi_llm.dpo_samples.task_executor.DpoSamplesOutputGenerator

  output_map:
    id:
      from: "id"
    taxonomy:
      from: "taxonomy"
    annotation_type:
      value: [
        "scale",
        "gpt4",
        "gpt-4o",
        "gpt-4o-mini"
      ]
    language:
      value: "en"
    tags:
      value: ["dpo_samples_rating"]
    conversation:
      from: "conversation"
      transform: "build_conversation"

  oasst_mapper:
    required: "no"
    type: "sft"
    intermediate_writing: "yes"
